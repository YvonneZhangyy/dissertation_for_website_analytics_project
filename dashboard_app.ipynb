{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dash\n",
        "from dash import dcc, html, dash_table\n",
        "from dash.dependencies import Input, Output, State\n",
        "import dash_bootstrap_components as dbc\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import threading\n",
        "import webbrowser\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "# Disable specific Matplotlib warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Glyph 0x21d2 not found\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"The default of observed=False is deprecated\")\n",
        "\n",
        "# --- Class and Function Definitions ---\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def extract_website_company(self, row):\n",
        "        \"\"\"Maps id_site to a website company name. This is a crucial step to distinguish the 'website company' being analyzed\n",
        "        from the 'visitor company' (ultimate_parent_name).\"\"\"\n",
        "        mapping = {\n",
        "            2: 'Company A',\n",
        "            3: 'Company B',\n",
        "            4: 'Company C',\n",
        "            5: 'Company D',\n",
        "            7: 'Company E'\n",
        "        }\n",
        "        return mapping.get(row['id_site'], 'Unknown')\n",
        "            \n",
        "    def load_and_preprocess_data(self):\n",
        "        \"\"\"Loads and preprocesses the raw traffic data.\"\"\"\n",
        "        try:\n",
        "            df_all = pd.read_csv(self.file_path)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {self.file_path}.\")\n",
        "            return pd.DataFrame()\n",
        "            \n",
        "        # 1. Core Data Cleaning and renaming columns\n",
        "        df_all = df_all.rename(columns=lambda x: x.strip())\n",
        "        df_all.columns = [col.replace('#', '').strip() for col in df_all.columns]\n",
        "            \n",
        "        df_all['section'] = df_all['section'].astype(str).str.strip()\n",
        "        df_all['sub-section'] = df_all['sub-section'].astype(str).str.strip()\n",
        "            \n",
        "        # Filter out rows with empty or 'nan' sections/subsections which are invalid for analysis\n",
        "        df_all = df_all[df_all['section'] != '']\n",
        "        df_all = df_all[df_all['sub-section'] != '']\n",
        "        df_all = df_all[df_all['section'].str.lower() != 'nan']\n",
        "        df_all = df_all[df_all['sub-section'].str.lower() != 'nan']\n",
        "        # Also remove 'Other' section as per report code\n",
        "        df_all = df_all[df_all['section'] != 'Other']\n",
        "\n",
        "\n",
        "        # 2. Filter out invalid data and duplicates to ensure data quality\n",
        "        # Only check columns that actually have variance\n",
        "        core_event_identifiers = ['id_visit', 'url', 'timestamp', 'download_flag', 'time_spent']\n",
        "        df_all = df_all.drop_duplicates(subset=core_event_identifiers, keep='first')\n",
        "            \n",
        "        # Convert timestamp and clean\n",
        "        df_all['timestamp'] = pd.to_datetime(df_all['timestamp'], errors='coerce')\n",
        "        df_all = df_all.dropna(subset=['timestamp']) # Drop rows where timestamp couldn't be parsed\n",
        "        df_all = df_all.sort_values(['id_visit', 'timestamp'])\n",
        "            \n",
        "\n",
        "        # 3. Add company information from the 'ultimate_parent_name' column\n",
        "        df_all['company'] = df_all['ultimate_parent_name'].fillna('Unknown')\n",
        "            \n",
        "        # NEW LOGIC: Extract the website company based on id_site mapping\n",
        "        df_all['website_company'] = df_all.apply(self.extract_website_company, axis=1)\n",
        "\n",
        "        # --- INTEGRATE NEW VISITOR SESSION IDENTIFICATION FROM REPORT CODE ---\n",
        "        # Ensure visitor_id is numeric and handle NaNs early for robust analysis\n",
        "        df_all['visitor_id'] = pd.to_numeric(df_all['visitor_id'], errors='coerce')\n",
        "            \n",
        "        # Crucially, filter out rows where visitor_id is NaN *before* grouping by visitor_id\n",
        "        df_filtered_visitors = df_all.dropna(subset=['visitor_id']).copy()\n",
        "\n",
        "        if df_filtered_visitors.empty:\n",
        "            print(\"Warning: No valid visitor_ids found after filtering. 'is_new_visitor_session' will be all False.\")\n",
        "            df_all['is_new_visitor_session'] = False\n",
        "        else:\n",
        "            # 1. Find the earliest visit timestamp for each visitor_id (indexed by visitor_id)\n",
        "            first_visit_time_per_visitor = df_filtered_visitors.groupby('visitor_id')['timestamp'].min()\n",
        "\n",
        "            # 2. For each session (id_visit), find its earliest timestamp (indexed by id_visit)\n",
        "            first_timestamp_per_visit = df_filtered_visitors.groupby('id_visit')['timestamp'].min()\n",
        "\n",
        "            # 3. For each session (id_visit), get its visitor_id (indexed by id_visit)\n",
        "            visitor_id_per_visit_map = df_filtered_visitors.groupby('id_visit')['visitor_id'].first()\n",
        "\n",
        "            # Map the 'first_visit_time_per_visitor' (indexed by visitor_id)\n",
        "            # onto the 'id_visit' index using the visitor_id_per_visit_map.\n",
        "            visitor_overall_first_timestamp_for_session = visitor_id_per_visit_map.map(first_visit_time_per_visitor)\n",
        "\n",
        "            # Now, compare the *session's first timestamp* with the *visitor's overall first timestamp*\n",
        "            # Both Series are now indexed by 'id_visit', allowing direct comparison.\n",
        "            is_first_session_for_visitor = (first_timestamp_per_visit == visitor_overall_first_timestamp_for_session)\n",
        "            \n",
        "            # Map this session-level flag back to the original (potentially unfiltered) df_all\n",
        "            df_all['is_new_visitor_session'] = df_all['id_visit'].map(is_first_session_for_visitor).fillna(False)\n",
        "            \n",
        "        # --- END NEW VISITOR SESSION IDENTIFICATION ---\n",
        "\n",
        "        return df_all\n",
        "\n",
        "class BehaviorAnalyzer:\n",
        "    # Features used for clustering and profiling visitors\n",
        "    CLUSTERING_FEATURES = [\n",
        "        'avg_session_depth', 'download_count', 'investor_interest_score',\n",
        "        'content_breadth', 'visit_count', 'is_repeat_visitor',\n",
        "        'has_download', 'has_ir', 'esg_visitor',\n",
        "        'ir_only_visitor', 'frequent_downloader', 'deep_path_visitor', 'is_high_intent'\n",
        "    ]\n",
        "\n",
        "    def __init__(self, dataframe):\n",
        "        self.df = dataframe.copy()\n",
        "    \n",
        "    def calculate_pageview_metrics(self):\n",
        "        \"\"\"Calculates key metrics related to pageviews and content engagement.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {}}\n",
        "            \n",
        "        pageviews_per_category = self.df['section'].value_counts()\n",
        "            \n",
        "        # Filter out pages with 0 time_spent for duration calculation, as these might be bounces or quick redirects\n",
        "        non_end_pages = self.df[self.df['time_spent'] > 0]\n",
        "        avg_duration_per_category = non_end_pages.groupby('section')['time_spent'].mean().sort_values(ascending=False)\n",
        "            \n",
        "        # \"End pages\" are defined as pages with 0 time_spent, potentially exit pages\n",
        "        end_pages = self.df[self.df['time_spent'] == 0]\n",
        "        end_page_count_per_category = end_pages['section'].value_counts()\n",
        "            \n",
        "        total_visits = self.df['id_visit'].nunique()\n",
        "        # End rate is the count of exit pages in a section divided by total visits (approximation of exit rate for sections)\n",
        "        end_rate_per_category = (end_page_count_per_category / total_visits).sort_values(ascending=False)\n",
        "            \n",
        "        alpha = 100 \n",
        "        real_interest_score = avg_duration_per_category.add(alpha * end_rate_per_category, fill_value=0).sort_values(ascending=False)\n",
        "            \n",
        "        metrics_data = {\n",
        "            'top_pageview_count': int(pageviews_per_category.iloc[0]) if not pageviews_per_category.empty else 0,\n",
        "            'top_pageview_section': pageviews_per_category.index[0] if not pageviews_per_category.empty else 'N/A',\n",
        "            'top_duration_section': avg_duration_per_category.index[0] if not avg_duration_per_category.empty else 'N/A',\n",
        "            'top_duration_time': round(avg_duration_per_category.iloc[0], 2) if not avg_duration_per_category.empty else 0.0,\n",
        "            'highest_end_rate_section': end_rate_per_category.index[0] if not end_rate_per_category.empty else 'N/A',\n",
        "            'highest_end_rate_value': round(end_rate_per_category.iloc[0], 4) if not end_rate_per_category.empty else 0.0,\n",
        "            'top_interest_score_section': real_interest_score.index[0] if not real_interest_score.empty else 'N/A',\n",
        "            'top_interest_score_value': round(real_interest_score.iloc[0], 2) if not real_interest_score.empty else 0.0,\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "\n",
        "    def analyze_session_path_length_and_repeat_visitors(self):\n",
        "        \"\"\"Calculates session length and identifies new vs. repeat visitors.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                'metrics_data': {\n",
        "                    'short_visits_count': 0,\n",
        "                    'deep_visits_count': 0,\n",
        "                    'repeat_visitors_count': 0,\n",
        "                    'new_visits_count': 0,\n",
        "                    'returning_visits_count': 0,\n",
        "                    'new_depth': 0.0,\n",
        "                    'returning_depth': 0.0\n",
        "                }\n",
        "            }\n",
        "            \n",
        "        visit_counts = self.df.groupby('id_visit').size() # Number of pageviews per visit\n",
        "        # Define thresholds for short/deep visits based on percentiles (e.g., 99th percentile for normal sessions)\n",
        "        session_length_threshold = visit_counts.quantile(0.99) if not visit_counts.empty else 0\n",
        "        normal_sessions = visit_counts[visit_counts <= session_length_threshold] if not visit_counts.empty else pd.Series(dtype='int64')\n",
        "        \n",
        "        # Ensure short_visits and deep_visits are calculated before being used.\n",
        "        # Short visits are sessions with only 1 pageview\n",
        "        short_visits = (normal_sessions == 1).sum() if not normal_sessions.empty else 0 \n",
        "        # Deep visits are sessions with 5 or more pageviews\n",
        "        deep_visits = (normal_sessions >= 5).sum() if not normal_sessions.empty else 0\n",
        "            \n",
        "        visitor_visit_counts = self.df.groupby('visitor_id')['id_visit'].nunique()\n",
        "        visit_count_threshold = visitor_visit_counts.quantile(0.99) if not visitor_visit_counts.empty else 0\n",
        "        normal_visitors = visitor_visit_counts[visitor_visit_counts <= visit_count_threshold] if not visitor_visit_counts.empty else pd.Series(dtype='int64')\n",
        "        repeat_visitors_count = (normal_visitors > 1).sum() # Visitors with more than one unique visit\n",
        "\n",
        "        # --- IMPORTANT: USE THE 'is_new_visitor_session' COLUMN CREATED IN DataLoader ---\n",
        "        new_visits = self.df[self.df['is_new_visitor_session']]\n",
        "        returning_visits = self.df[~self.df['is_new_visitor_session']]\n",
        "            \n",
        "        new_visits_unique_count = new_visits['id_visit'].nunique()\n",
        "        returning_visits_unique_count = returning_visits['id_visit'].nunique()\n",
        "            \n",
        "        new_depth = new_visits.groupby('id_visit').size().mean() if new_visits_unique_count > 0 else 0.0\n",
        "        returning_depth = returning_visits.groupby('id_visit').size().mean() if returning_visits_unique_count > 0 else 0.0\n",
        "            \n",
        "        metrics_data = {\n",
        "            'short_visits_count': int(short_visits),\n",
        "            'deep_visits_count': int(deep_visits),\n",
        "            'repeat_visitors_count': int(repeat_visitors_count),\n",
        "            'new_visits_count': int(new_visits_unique_count),\n",
        "            'returning_visits_count': int(returning_visits_unique_count),\n",
        "            'new_depth': round(new_depth, 2),\n",
        "            'returning_depth': round(returning_depth, 2)\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def analyze_most_common_paths(self):\n",
        "        \"\"\"Identifies and counts the most common sequential paths taken by visitors.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'top_common_paths': {}, 'top_common_path_string': 'N/A', 'top_common_path_count': 0}}\n",
        "            \n",
        "        df_sorted = self.df.sort_values(['id_visit', 'timestamp'])\n",
        "        # Concatenate sections within each visit to form a path string\n",
        "        df_sorted['path'] = df_sorted.groupby('id_visit')['section'].transform(lambda x: ' -> '.join(x.astype(str)))\n",
        "            \n",
        "        path_counts = df_sorted['path'].value_counts()\n",
        "        top_paths = path_counts.head(10) # Get top 10 most common paths\n",
        "            \n",
        "        metrics_data = {\n",
        "            'top_common_paths': top_paths.to_dict(),\n",
        "            'top_common_path_string': top_paths.index[0] if not top_paths.empty else 'N/A',\n",
        "            'top_common_path_count': int(top_paths.iloc[0]) if not top_paths.empty else 0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "\n",
        "    def compare_new_vs_returning(self):\n",
        "        \"\"\"Compares engagement metrics (depth, bounce rate) for new vs. returning visitors.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'new_depth_comparison': 0.0, 'returning_depth_comparison': 0.0, 'new_bounce_rate_comparison': 0.0, 'returning_bounce_rate_comparison': 0.0}}\n",
        "            \n",
        "        # --- IMPORTANT: USE THE 'is_new_visitor_session' COLUMN CREATED IN DataLoader ---\n",
        "        new_visits_df = self.df[self.df['is_new_visitor_session']]\n",
        "        returning_visits_df = self.df[~self.df['is_new_visitor_session']]\n",
        "            \n",
        "        new_depth = new_visits_df.groupby('id_visit').size().mean() if not new_visits_df.empty else 0.0\n",
        "        returning_depth = returning_visits_df.groupby('id_visit').size().mean() if not returning_visits_df.empty else 0.0\n",
        "            \n",
        "        # Recursively call bounce rate calculation for new and returning segments\n",
        "        new_bounce_result = BehaviorAnalyzer(new_visits_df).calculate_bounce_rate()\n",
        "        returning_bounce_result = BehaviorAnalyzer(returning_visits_df).calculate_bounce_rate()\n",
        "            \n",
        "        metrics_data = {\n",
        "            'new_depth_comparison': round(new_depth, 2),\n",
        "            'returning_depth_comparison': round(returning_depth, 2),\n",
        "            'new_bounce_rate_comparison': round(new_bounce_result['metrics_data']['overall_bounce_rate'], 4),\n",
        "            'returning_bounce_rate_comparison': round(returning_bounce_result['metrics_data']['overall_bounce_rate'], 4),\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def find_optimal_clusters(self, X_scaled, k_range=range(3, 11)):\n",
        "        \"\"\"\n",
        "        Uses the elbow method and silhouette score to suggest an optimal number of clusters.\n",
        "        Returns the optimal n_clusters and a DataFrame of scores.\n",
        "        \"\"\"\n",
        "        if X_scaled.shape[0] <= max(k_range):\n",
        "            return None, \"Not enough data points to test the full range of clusters.\"\n",
        "        \n",
        "        distortions = []\n",
        "        silhouette_scores = []\n",
        "        \n",
        "        for i in k_range:\n",
        "            kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
        "            kmeans.fit(X_scaled)\n",
        "            distortions.append(kmeans.inertia_)\n",
        "            \n",
        "            if i > 1:\n",
        "                score = silhouette_score(X_scaled, kmeans.labels_)\n",
        "                silhouette_scores.append(score)\n",
        "            else: \n",
        "                silhouette_scores.append(np.nan)\n",
        "\n",
        "        elbow_index = np.argmin(np.diff(distortions, 2)) + 2\n",
        "        \n",
        "        if not silhouette_scores:\n",
        "            best_silhouette_k = k_range[0] \n",
        "        else:\n",
        "            best_silhouette_k = k_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "        score_df = pd.DataFrame({\n",
        "            'n_clusters': list(k_range),\n",
        "            'Distortion (Inertia)': distortions,\n",
        "            'Silhouette Score': silhouette_scores \n",
        "        })\n",
        "\n",
        "        optimal_n_clusters = best_silhouette_k\n",
        "        \n",
        "        return optimal_n_clusters, score_df\n",
        "\n",
        "    def compare_with_gmm(self, X_scaled, n_clusters, max_gmm_components=10):\n",
        "        \"\"\"\n",
        "        Compares K-Means with a Gaussian Mixture Model (GMM) using BIC.\n",
        "        Returns the BIC scores for both models.\n",
        "        \"\"\"\n",
        "        from sklearn.mixture import GaussianMixture\n",
        "        \n",
        "        if X_scaled.shape[0] < n_clusters or X_scaled.shape[0] < 2:\n",
        "            return {'kmeans_bic': np.nan, 'gmm_bic': np.nan}\n",
        "\n",
        "        # K-Means is not natively compared with BIC, but we can compute it manually\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
        "        kmeans.fit(X_scaled)\n",
        "        kmeans_labels = kmeans.labels_\n",
        "        kmeans_bic = np.nan # BIC is not ideal for K-Means, but can be approximated.\n",
        "                        # Let's just state this in the analysis.\n",
        "        \n",
        "        # GMM naturally supports BIC. Iterate to find the best GMM.\n",
        "        bic_scores = []\n",
        "        for i in range(1, min(max_gmm_components, X_scaled.shape[0] + 1)):\n",
        "            try:\n",
        "                gmm = GaussianMixture(n_components=i, random_state=42, n_init=10)\n",
        "                gmm.fit(X_scaled)\n",
        "                bic_scores.append(gmm.bic(X_scaled))\n",
        "            except ValueError: # Occurs if n_components is too large for the data\n",
        "                bic_scores.append(np.nan)\n",
        "                \n",
        "        best_gmm_bic = min(bic_scores) if bic_scores else np.nan\n",
        "        best_gmm_n_components = np.argmin(bic_scores) + 1 if bic_scores else np.nan\n",
        "        \n",
        "        return {'kmeans_bic': kmeans_bic, 'best_gmm_bic': best_gmm_bic, 'best_gmm_n_components': best_gmm_n_components}\n",
        "\n",
        "    def perform_clustering_and_profiling(self, n_clusters=5):\n",
        "        \"\"\"\n",
        "        Performs K-Means clustering on visitor profiles with enhancements from the paper.\n",
        "        Now includes a search for the optimal number of clusters and GMM comparison.\n",
        "        \"\"\"\n",
        "        insights = []\n",
        "        metrics_data = {'cluster_summary': {}}\n",
        "        \n",
        "        visitor_profiles_results = self.generate_visitor_profiles()\n",
        "        visitor_profiles_df = visitor_profiles_results['visitor_profiles_df']\n",
        "        \n",
        "        features_for_clustering = [f for f in self.CLUSTERING_FEATURES if f in visitor_profiles_df.columns]\n",
        "        X = visitor_profiles_df[features_for_clustering].copy()\n",
        "        X = X.fillna(0)\n",
        "        \n",
        "        if X.empty or X.shape[0] < 2:\n",
        "            warning_msg = \"Warning: Not enough valid visitor profiles to perform clustering.\"\n",
        "            insights.append(warning_msg)\n",
        "            return {'visitor_profiles_with_clusters': visitor_profiles_df, 'cluster_descriptions': {}, 'insights': insights, 'metrics_data': metrics_data}\n",
        "\n",
        "        # --- Robustness check for zero-variance features ---\n",
        "        zero_variance_cols = X.columns[X.std() < 1e-9]\n",
        "        if not zero_variance_cols.empty:\n",
        "            warning_msg = f\"Warning: Features with zero variance found and filtered: {list(zero_variance_cols)}\"\n",
        "            insights.append(warning_msg)\n",
        "            X = X.drop(columns=zero_variance_cols)\n",
        "            \n",
        "        if X.shape[1] == 0:\n",
        "            warning_msg = \"Warning: All features had zero variance. Cannot perform clustering.\"\n",
        "            insights.append(warning_msg)\n",
        "            return {'visitor_profiles_with_clusters': visitor_profiles_df, 'cluster_descriptions': {}, 'insights': insights, 'metrics_data': metrics_data}\n",
        "            \n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # --- Find optimal number of clusters using elbow & silhouette scores ---\n",
        "        optimal_n_clusters_found, score_metrics = self.find_optimal_clusters(X_scaled)\n",
        "        if isinstance(score_metrics, pd.DataFrame):\n",
        "            insights.append(f\"Optimal cluster number suggested by silhouette analysis is: {optimal_n_clusters_found}\")\n",
        "            metrics_data['clustering_scores'] = score_metrics.to_dict('records')\n",
        "        else:\n",
        "            insights.append(f\"Warning: Could not determine optimal cluster number: {score_metrics}\")\n",
        "            optimal_n_clusters_found = n_clusters # Fall back to the default\n",
        "        \n",
        "        n_clusters_to_use = min(optimal_n_clusters_found, X.shape[0]-1) if optimal_n_clusters_found else n_clusters\n",
        "        \n",
        "        # ---  Integrate GMM comparison to complement K-Means ---\n",
        "        if X_scaled.shape[0] > 1 and n_clusters_to_use > 1:\n",
        "            try:\n",
        "                gmm_results = self.compare_with_gmm(X_scaled, n_clusters_to_use)\n",
        "                if pd.notna(gmm_results['best_gmm_bic']):\n",
        "                    insights.append(f\"GMM analysis suggests a best fit with {gmm_results['best_gmm_n_components']} components (BIC: {gmm_results['best_gmm_bic']:.2f}). This provides a comparative check for the K-Means clustering assumptions.\")\n",
        "                    metrics_data['gmm_comparison'] = gmm_results\n",
        "            except Exception as e:\n",
        "                insights.append(f\"Warning: GMM comparison failed due to an error: {e}\")\n",
        "\n",
        "        if n_clusters_to_use <= 1:\n",
        "            insights.append(\"Clustering resulted in only one cluster. No meaningful segmentation.\")\n",
        "            visitor_profiles_df['cluster'] = 0\n",
        "            return {'visitor_profiles_with_clusters': visitor_profiles_df, 'cluster_descriptions': {}, 'insights': insights, 'metrics_data': metrics_data}\n",
        "\n",
        "\n",
        "        # The rest of your existing K-Means, t-SNE, and persona generation logic...\n",
        "        kmeans = KMeans(n_clusters=n_clusters_to_use, random_state=42, n_init=10)\n",
        "        visitor_profiles_df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "        \n",
        "        cluster_centers_scaled = kmeans.cluster_centers_\n",
        "        cluster_centers = pd.DataFrame(scaler.inverse_transform(cluster_centers_scaled), columns=X.columns)\n",
        "        cluster_descriptions = {}\n",
        "\n",
        "        for i in range(n_clusters_to_use):\n",
        "            cluster_data = cluster_centers.iloc[i]\n",
        "            desc_parts = []\n",
        "            cluster_name = f\"Cluster {i}\"\n",
        "            \n",
        "          \n",
        "            if i == 0:\n",
        "                cluster_name = \"Highly Engaged Core Investors\"\n",
        "            elif i == 1:\n",
        "                cluster_name = \"Low-Engagement New Visitors\"\n",
        "            elif i == 2:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            elif i == 3:\n",
        "                cluster_name = \"High-Engagement Core Investors\"\n",
        "            elif i == 4:\n",
        "                cluster_name = \"Low-Engagement Core Investors\"\n",
        "            elif i == 5:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            elif i == 6:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            elif i == 7:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            elif i == 8:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            elif i == 9:\n",
        "                cluster_name = \"High-Engagement New Visitors\"\n",
        "            if 'avg_session_depth' in cluster_data: desc_parts.append(f\"Avg session depth: {cluster_data['avg_session_depth']:.1f} pages\")\n",
        "            \n",
        "            full_description = f\"**{cluster_name}:** \" + \" | \".join(desc_parts) + \".\"\n",
        "            cluster_descriptions[f'Cluster {i}'] = full_description\n",
        "            metrics_data['cluster_summary'][f'Cluster {i}'] = cluster_data.to_dict()\n",
        "\n",
        "        try:\n",
        "            if X_scaled.shape[0] > 1 and X_scaled.shape[1] >= 2 and n_clusters_to_use > 1:\n",
        "                perplexity = min(30, X_scaled.shape[0] - 1)\n",
        "                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "                X_tsne = tsne.fit_transform(X_scaled)\n",
        "                visitor_profiles_df['tsne_x'] = X_tsne[:, 0]\n",
        "                visitor_profiles_df['tsne_y'] = X_tsne[:, 1]\n",
        "            else:\n",
        "                insights.append(\"Warning: Not enough data points or features to perform t-SNE visualization for clustering.\")\n",
        "        except Exception as e:\n",
        "            insights.append(f\"Error: Could not visualize clusters using t-SNE: {e}.\")\n",
        "\n",
        "        return {\n",
        "            'visitor_profiles_with_clusters': visitor_profiles_df,\n",
        "            'cluster_descriptions': cluster_descriptions,\n",
        "            'insights': insights,\n",
        "            'metrics_data': metrics_data\n",
        "        }\n",
        "\n",
        "    def compare_by_country(self, top_n_countries=5):\n",
        "        \"\"\"Compares engagement metrics across top visitor countries.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'highest_bounce_country': 'N/A', 'highest_bounce_country_rate': 0.0, 'deepest_engagement_country': 'N/A', 'deepest_engagement_country_depth': 0.0}}\n",
        "            \n",
        "        country_pageviews = self.df.groupby('location_country')['id_visit'].count().sort_values(ascending=False)\n",
        "        top_countries = country_pageviews.head(top_n_countries).index.tolist()\n",
        "            \n",
        "        country_bounce_rates = {}\n",
        "        country_avg_depth = {}\n",
        "            \n",
        "        for country in top_countries:\n",
        "            country_df = self.df[self.df['location_country'] == country]\n",
        "            if not country_df.empty:\n",
        "                country_analyzer = BehaviorAnalyzer(country_df)\n",
        "                country_bounce_result = country_analyzer.calculate_bounce_rate()\n",
        "                country_bounce_rates[country] = country_bounce_result['metrics_data']['overall_bounce_rate']\n",
        "                \n",
        "                country_visits = country_df.groupby('id_visit').size()\n",
        "                country_avg_depth[country] = country_visits.mean() if len(country_visits) > 0 else 0.0\n",
        "            \n",
        "        bounce_df = pd.DataFrame(list(country_bounce_rates.items()), columns=['Country', 'Bounce_Rate']).sort_values(by='Bounce_Rate', ascending=False)\n",
        "        depth_df = pd.DataFrame(list(country_avg_depth.items()), columns=['Country', 'Avg_Depth']).sort_values(by='Avg_Depth', ascending=False)\n",
        "            \n",
        "        metrics_data = {\n",
        "            'highest_bounce_country': bounce_df.iloc[0]['Country'] if not bounce_df.empty else 'N/A',\n",
        "            'highest_bounce_country_rate': round(bounce_df.iloc[0]['Bounce_Rate'], 4) if not bounce_df.empty else 0.0,\n",
        "            'deepest_engagement_country': depth_df.iloc[0]['Country'] if not depth_df.empty else 'N/A',\n",
        "            'deepest_engagement_country_depth': round(depth_df.iloc[0]['Avg_Depth'], 2) if not depth_df.empty else 0.0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def calculate_bounce_rate(self):\n",
        "        \"\"\"Calculates the overall bounce rate and bounce rate per section.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'overall_bounce_rate': 0.0, 'highest_bounce_section': 'N/A', 'highest_bounce_value': 0.0}}\n",
        "            \n",
        "        visit_counts = self.df.groupby('id_visit').size() \n",
        "        bounces = visit_counts[visit_counts == 1].count()\n",
        "        total_visits = visit_counts.count()\n",
        "            \n",
        "        overall_bounce_rate = bounces / total_visits if total_visits > 0 else 0.0\n",
        "            \n",
        "        section_bounce_rates = {}\n",
        "        for section, group in self.df.groupby('section'):\n",
        "            section_visit_counts = group.groupby('id_visit').size()\n",
        "            section_bounces = section_visit_counts[section_visit_counts == 1].count()\n",
        "            section_total_visits = section_visit_counts.count()\n",
        "            bounce_rate = section_bounces / section_total_visits if section_total_visits > 0 else 0\n",
        "            section_bounce_rates[section] = bounce_rate\n",
        "            \n",
        "        bounce_df = pd.DataFrame(list(section_bounce_rates.items()), columns=['section', 'bounce_rate']).sort_values(by='bounce_rate', ascending=False)\n",
        "            \n",
        "        metrics_data = {\n",
        "            'overall_bounce_rate': round(overall_bounce_rate, 4),\n",
        "            'highest_bounce_section': bounce_df.iloc[0]['section'] if not bounce_df.empty else 'N/A',\n",
        "            'highest_bounce_value': round(bounce_df.iloc[0]['bounce_rate'], 4) if not bounce_df.empty else 0.0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "\n",
        "    def analyze_download_behavior(self):\n",
        "        \"\"\"Analyzes user download behavior across sections and subsections.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'top_download_section': 'N/A', 'top_download_count': 0, 'top_download_subsection': 'N/A', 'top_download_subsection_count': 0, 'top_section_with_downloads': 'N/A', 'top_section_with_downloads_count': 0, 'top_conv_subsection': 'N/A', 'top_conv_rate': 0.0}}\n",
        "            \n",
        "        download_pages = self.df[self.df['download_flag'] == True]\n",
        "        download_section_counts = download_pages['section'].value_counts()\n",
        "        download_subsection_counts = download_pages['sub-section'].value_counts()\n",
        "            \n",
        "        # Identify visits that contain any download\n",
        "        download_id_visits = self.df[self.df['download_flag'] == True]['id_visit'].unique()\n",
        "        related_visits = self.df[self.df['id_visit'].isin(download_id_visits)]\n",
        "        section_counts_with_downloads = related_visits['section'].value_counts()\n",
        "            \n",
        "        # Calculate download conversion rate per subsection\n",
        "        subsection_downloads = download_pages['sub-section'].value_counts()\n",
        "        subsection_total = self.df['sub-section'].value_counts()\n",
        "        subsection_download_rate = (subsection_downloads / subsection_total).dropna() # Handle cases where denominator is zero\n",
        "            \n",
        "        metrics_data = {\n",
        "            'top_download_section': download_section_counts.index[0] if not download_section_counts.empty else 'N/A',\n",
        "            'top_download_count': int(download_section_counts.iloc[0]) if not download_section_counts.empty else 0,\n",
        "            'top_download_subsection': download_subsection_counts.index[0] if not download_subsection_counts.empty else 'N/A',\n",
        "            'top_download_subsection_count': int(download_subsection_counts.iloc[0]) if not download_subsection_counts.empty else 0,\n",
        "            'top_section_with_downloads': section_counts_with_downloads.index[0] if not section_counts_with_downloads.empty else 'N/A',\n",
        "            'top_section_with_downloads_count': int(section_counts_with_downloads.iloc[0]) if not section_counts_with_downloads.empty else 0,\n",
        "            'top_conv_subsection': subsection_download_rate.index[0] if not subsection_download_rate.empty else 'N/A',\n",
        "            'top_conv_rate': round(subsection_download_rate.iloc[0], 4) if not subsection_download_rate.empty else 0.0,\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def analyze_content_interest_by_cluster(self):\n",
        "        if self.df.empty:\n",
        "            return pd.DataFrame()\n",
        "        if 'cluster' not in self.df.columns:\n",
        "            print(\"Warning: 'cluster' column not found in DataFrame for analyze_content_interest_by_cluster.\")\n",
        "            return pd.DataFrame()\n",
        "        cluster_content_metrics = self.df.groupby(['cluster', 'section']).agg(\n",
        "            pageviews=('id_visit', 'count'),\n",
        "            total_time_spent=('time_spent', 'sum'),\n",
        "            downloads=('download_flag', lambda x: x.sum() if pd.api.types.is_bool_dtype(x) else (x == True).sum()),\n",
        "            unique_visitors=('visitor_id', 'nunique')\n",
        "        ).reset_index()\n",
        "\n",
        "        time_weight = 0.1\n",
        "        download_weight = 5.0 \n",
        "\n",
        "        cluster_content_metrics['interest_score'] = (\n",
        "            cluster_content_metrics['pageviews'] +\n",
        "            (cluster_content_metrics['total_time_spent'] * time_weight) +\n",
        "            (cluster_content_metrics['downloads'] * download_weight)\n",
        "        )\n",
        "\n",
        "        top_sections_per_cluster = []\n",
        "        for cluster_id in cluster_content_metrics['cluster'].unique():\n",
        "            cluster_data = cluster_content_metrics[cluster_content_metrics['cluster'] == cluster_id].sort_values(by='interest_score', ascending=False)\n",
        "            top_3_sections = cluster_data.head(3)\n",
        "            for idx, row in top_3_sections.iterrows():\n",
        "                top_sections_per_cluster.append({\n",
        "                    'Cluster ID': cluster_id,\n",
        "                    'Section': row['section'],\n",
        "                    'Interest Score': round(row['interest_score'], 2),\n",
        "                    'Rank': len(top_sections_per_cluster) % 3 + 1 # Rank within this cluster's top sections\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(top_sections_per_cluster)\n",
        "        \n",
        "    def perform_funnel_analysis(self):\n",
        "        \"\"\"Performs a simple funnel analysis from home page to download.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'funnel_conversion_rate': 0.0, 'funnel_start_visits': 0, 'funnel_converted_visits': 0}}\n",
        "            \n",
        "        # Determine the first page of each visit (the landing page)\n",
        "        df_sorted = self.df.sort_values(['id_visit', 'timestamp'])\n",
        "        landing_pages = df_sorted.groupby('id_visit').first().reset_index()\n",
        "\n",
        "        # Get the unique visitors who started their session on a home page\n",
        "        home_visits = landing_pages[landing_pages['section'].str.lower() == 'home page']['id_visit'].unique()\n",
        "            \n",
        "        # Get the unique visitors from the 'home_visits' group who also have a download\n",
        "        download_visits = self.df[(self.df['id_visit'].isin(home_visits)) & (self.df['download_flag'] == True)]['id_visit'].unique()\n",
        "            \n",
        "        # Calculate the funnel conversion rate\n",
        "        funnel_rate = len(download_visits) / len(home_visits) if len(home_visits) > 0 else 0.0\n",
        "            \n",
        "        metrics_data = {\n",
        "            'funnel_conversion_rate': round(funnel_rate, 4),\n",
        "            'funnel_start_visits': int(len(home_visits)),\n",
        "            'funnel_converted_visits': int(len(download_visits))\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def generate_visitor_profiles(self):\n",
        "        \"\"\"Generates detailed profiles for each unique visitor, including derived behavioral features and tags.\"\"\"\n",
        "        df_profiling = self.df.dropna(subset=['visitor_id'])\n",
        "        if df_profiling.empty:\n",
        "            empty_profiles = pd.DataFrame(columns=['visitor_id'] + self.CLUSTERING_FEATURES + ['top_visited_section', 'user_tags', 'company'])\n",
        "            return {'visitor_profiles_df': empty_profiles, 'metrics_data': {}}\n",
        "            \n",
        "        all_visitor_ids = df_profiling['visitor_id'].unique()\n",
        "            \n",
        "        # Link company info to visitor_id (take the first company associated if multiple exist)\n",
        "        visitor_company = df_profiling.groupby('visitor_id')['company'].first().rename('company')\n",
        "\n",
        "        # Calculate various behavioral features for each visitor\n",
        "        session_depth = df_profiling.groupby(['visitor_id', 'id_visit']).size().groupby('visitor_id').mean().rename('avg_session_depth')\n",
        "        download_count = df_profiling[df_profiling['download_flag'] == True].groupby('visitor_id').size().rename('download_count')\n",
        "            \n",
        "        # Determine the most visited section for each visitor\n",
        "        top_section_raw = df_profiling.groupby(['visitor_id', 'section']).size().reset_index(name='count')\n",
        "        top_section_raw = top_section_raw.sort_values(['visitor_id', 'count'], ascending=[True, False])\n",
        "        top_section = top_section_raw.groupby('visitor_id').first()['section'].rename('top_visited_section')\n",
        "            \n",
        "        # Calculate investor interest score based on IR section pageviews\n",
        "        ir_score = df_profiling[df_profiling['section'].str.contains('Investor Relations', na=False, case=False)].groupby('visitor_id').size().rename('investor_interest_score')\n",
        "        ir_score = ir_score.reindex(all_visitor_ids, fill_value=0) # Fill missing visitors with 0 score\n",
        "            \n",
        "        content_breadth = df_profiling.groupby('visitor_id')['section'].nunique().rename('content_breadth')\n",
        "        content_breadth = content_breadth.reindex(all_visitor_ids, fill_value=0)\n",
        "            \n",
        "        visit_count_series = df_profiling.groupby('visitor_id')['id_visit'].nunique().rename('visit_count')\n",
        "        visit_count_series = visit_count_series.reindex(all_visitor_ids, fill_value=0)\n",
        "            \n",
        "        # --- RE-CALCULATE short_visits and deep_visits for generate_visitor_profiles ---\n",
        "        # This resolves the NameError for short_visits and deep_visits\n",
        "        visit_counts_for_profiles = df_profiling.groupby('id_visit').size()\n",
        "        session_length_threshold_for_profiles = visit_counts_for_profiles.quantile(0.99) if not visit_counts_for_profiles.empty else 0\n",
        "        normal_sessions_for_profiles = visit_counts_for_profiles[visit_counts_for_profiles <= session_length_threshold_for_profiles] if not visit_counts_for_profiles.empty else pd.Series(dtype='int64')\n",
        "        short_visits_calc = (normal_sessions_for_profiles == 1).sum() if not normal_sessions_for_profiles.empty else 0 \n",
        "        deep_visits_calc = (normal_sessions_for_profiles >= 5).sum() if not normal_sessions_for_profiles.empty else 0\n",
        "        \n",
        "        # --- IMPORTANT: USE THE 'is_new_visitor_session' COLUMN CREATED IN DataLoader ---\n",
        "        # This resolves the NameError for is_new_visitor_session_series and is_repeat_visitor\n",
        "        is_new_visitor_session_series = df_profiling.groupby('visitor_id')['is_new_visitor_session'].apply(lambda x: (~x).any()).astype(int).rename('is_repeat_visitor')\n",
        "        is_new_visitor_session_series = is_new_visitor_session_series.reindex(all_visitor_ids, fill_value=0)\n",
        "\n",
        "        # --- Re-defined has_download, has_ir, and esg_visitor_flag ---\n",
        "        # This resolves the NameError for these variables.\n",
        "        has_download = (download_count > 0).astype(int).rename('has_download').reindex(all_visitor_ids, fill_value=0)\n",
        "        has_ir = (ir_score >= 2).astype(int).rename('has_ir').reindex(all_visitor_ids, fill_value=0)\n",
        "        esg_visitors_in_profile = df_profiling[df_profiling['section'].str.contains('ESG', na=False, case=False)]['visitor_id'].unique()\n",
        "        esg_visitor_flag = pd.Series(all_visitor_ids).isin(esg_visitors_in_profile).astype(int).set_axis(all_visitor_ids).rename('esg_visitor')\n",
        "\n",
        "\n",
        "        # Combine all features into a single DataFrame\n",
        "        visitor_profiles_df = pd.concat([\n",
        "            session_depth, download_count, top_section, ir_score, content_breadth,\n",
        "            visit_count_series, is_new_visitor_session_series, has_download, has_ir, esg_visitor_flag, visitor_company\n",
        "        ], axis=1).reset_index(names=['visitor_id'])\n",
        "            \n",
        "        # Ensure all clustering features are numeric and handle NaNs\n",
        "        for col in self.CLUSTERING_FEATURES:\n",
        "            if col in visitor_profiles_df.columns:\n",
        "                visitor_profiles_df[col] = pd.to_numeric(visitor_profiles_df[col], errors='coerce').fillna(0)\n",
        "            \n",
        "        # Derive composite behavioral flags\n",
        "        visitor_profiles_df['is_high_intent'] = ((visitor_profiles_df['has_download'] > 0) & (visitor_profiles_df['has_ir'] > 0)).astype(int)\n",
        "        visitor_profiles_df['ir_only_visitor'] = ((visitor_profiles_df['content_breadth'] == 1) & (visitor_profiles_df['top_visited_section'] == 'Investor Relations')).astype(int)\n",
        "        visitor_profiles_df['frequent_downloader'] = (visitor_profiles_df['download_count'] > 3).astype(int)\n",
        "        visitor_profiles_df['deep_path_visitor'] = (visitor_profiles_df['avg_session_depth'] > 5).astype(int)\n",
        "            \n",
        "        # Generate user tags based on derived features\n",
        "        def get_user_tags(row):\n",
        "            tags = []\n",
        "            if row['ir_only_visitor']: tags.append('IR-Only Browser')\n",
        "            if row['frequent_downloader']: tags.append('Frequent Downloader')\n",
        "            if row['deep_path_visitor']: tags.append('Deep Path Visitor')\n",
        "            if row['esg_visitor'] > 0: tags.append('ESG-Focused')\n",
        "            if row['avg_session_depth'] <= 1.1 and row['avg_session_depth'] > 0: tags.append('High Bounce Rate') # Very low depth indicates high bounce\n",
        "            if row['is_high_intent']: tags.append('High-Intent')\n",
        "            if row['is_repeat_visitor']: tags.append('Repeat Visitor')\n",
        "            else: tags.append('New Visitor')\n",
        "            return ','.join(tags)\n",
        "            \n",
        "        visitor_profiles_df['user_tags'] = visitor_profiles_df.apply(get_user_tags, axis=1)\n",
        "        visitor_profiles_df = visitor_profiles_df.dropna(subset=['visitor_id']) \n",
        "            \n",
        "        if not visitor_profiles_df.empty:\n",
        "            visitor_profiles_df['visitor_id'] = visitor_profiles_df['visitor_id'].astype(int)\n",
        "\n",
        "        # Aggregate metrics for profiling insights\n",
        "        num_high_intent = visitor_profiles_df['is_high_intent'].sum()\n",
        "        num_frequent_downloaders = visitor_profiles_df['frequent_downloader'].sum()\n",
        "        num_ir_only = visitor_profiles_df['ir_only_visitor'].sum()\n",
        "        num_esg_visitors = visitor_profiles_df['esg_visitor'].sum()\n",
        "            \n",
        "        metrics_data = {\n",
        "            'short_visits_count': int(short_visits_calc), \n",
        "            'deep_visits_count': int(deep_visits_calc),\n",
        "            'num_high_intent_users': int(num_high_intent),\n",
        "            'num_frequent_downloaders': int(num_frequent_downloaders),\n",
        "            'num_ir_only_visitors': int(num_ir_only),\n",
        "            'num_esg_visitors': int(num_esg_visitors),\n",
        "        }\n",
        "        return {'visitor_profiles_df': visitor_profiles_df, 'metrics_data': metrics_data}\n",
        "\n",
        "    def calculate_average_depth(self):\n",
        "        \"\"\"Calculates the average number of pages viewed per visit.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'avg_depth_per_visit': 0.0}}\n",
        "        visit_counts = self.df.groupby('id_visit').size() # Number of pageviews per visit\n",
        "        avg_depth_per_visit = visit_counts.mean() if not visit_counts.empty else 0.0\n",
        "        metrics_data = {'avg_depth_per_visit': round(avg_depth_per_visit, 2)}\n",
        "        return {'metrics_data': metrics_data}\n",
        "\n",
        "    def analyze_unique_investors(self):\n",
        "        \"\"\"Identifies top sections by unique investor (visitor_id) count.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'top_unique_investor_section': 'N/A', 'top_unique_investor_count': 0}}\n",
        "        unique_investor_per_category = self.df.groupby('section')['visitor_id'].nunique()\n",
        "        metrics_data = {\n",
        "            'top_unique_investor_section': unique_investor_per_category.idxmax() if not unique_investor_per_category.empty else 'N/A',\n",
        "            'top_unique_investor_count': int(unique_investor_per_category.max()) if not unique_investor_per_category.empty else 0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "\n",
        "    def analyze_sub_section_details(self):\n",
        "        \"\"\"Provides detailed metrics for subsections.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'top_sub_pv': 'N/A', 'top_sub_pv_count': 0, 'top_sub_duration': 'N/A', 'top_sub_duration_time': 0.0, 'top_report_visitor_type': 'N/A', 'top_report_visitor_count': 0}}\n",
        "            \n",
        "        subsection_pageviews = self.df['sub-section'].value_counts()\n",
        "        non_end_pages = self.df[self.df['time_spent'] > 0]\n",
        "        avg_duration_subsection = non_end_pages.groupby('sub-section')['time_spent'].mean()\n",
        "            \n",
        "        # Analyze specific 'Reports & Presentations' subsection for visitor companies\n",
        "        report_visitors = self.df[self.df['sub-section'] == 'Reports & Presentations']['ultimate_parent_name'].value_counts()\n",
        "            \n",
        "        metrics_data = {\n",
        "            'top_sub_pv': subsection_pageviews.index[0] if not subsection_pageviews.empty else 'N/A',\n",
        "            'top_sub_pv_count': int(subsection_pageviews.iloc[0]) if not subsection_pageviews.empty else 0,\n",
        "            'top_sub_duration': avg_duration_subsection.idxmax() if not avg_duration_subsection.empty else 'N/A',\n",
        "            'top_sub_duration_time': round(avg_duration_subsection.max(), 2) if not avg_duration_subsection.empty else 0.0,\n",
        "            'top_report_visitor_type': report_visitors.index[0] if not report_visitors.empty else 'N/A',\n",
        "            'top_report_visitor_count': int(report_visitors.iloc[0]) if not report_visitors.empty else 0,\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "    \n",
        "    def analyze_session_paths(self):\n",
        "        \"\"\"Identifies and counts the most common sequential paths taken by visitors.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'top_common_paths': {}, 'top_common_path_string': 'N/A', 'top_common_path_count': 0}}\n",
        "            \n",
        "        df_sorted = self.df.sort_values(['id_visit', 'timestamp'])\n",
        "        df_sorted['path'] = df_sorted.groupby('id_visit')['section'].transform(lambda x: ' -> '.join(x.astype(str)))\n",
        "            \n",
        "        path_counts = df_sorted['path'].value_counts()\n",
        "        top_paths = path_counts.head(10) \n",
        "            \n",
        "        metrics_data = {\n",
        "            'top_common_paths': top_paths.to_dict(),\n",
        "            'top_common_path_string': top_paths.index[0] if not top_paths.empty else 'N/A',\n",
        "            'top_common_path_count': int(top_paths.iloc[0]) if not top_paths.empty else 0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "        \n",
        "    def analyze_time_distribution(self):\n",
        "        \"\"\"Analyzes visitor activity by hour of day and day of week.\"\"\"\n",
        "        if self.df.empty:\n",
        "            return {'metrics_data': {'peak_hour': 0, 'peak_hour_count': 0, 'peak_weekday': 'N/A', 'peak_weekday_count': 0}}\n",
        "            \n",
        "        hourly_counts = self.df['timestamp'].dt.hour.value_counts().sort_index()\n",
        "        weekday_counts = self.df['timestamp'].dt.dayofweek.value_counts().sort_index()\n",
        "        weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "            \n",
        "        metrics_data = {\n",
        "            'peak_hour': int(hourly_counts.idxmax()) if not hourly_counts.empty else 0,\n",
        "            'peak_hour_count': int(hourly_counts.max()) if not hourly_counts.empty else 0,\n",
        "            'peak_weekday': weekdays[weekday_counts.idxmax()] if not weekday_counts.empty else 'N/A',\n",
        "            'peak_weekday_count': int(weekday_counts.max()) if not weekday_counts.empty else 0\n",
        "        }\n",
        "        return {'metrics_data': metrics_data}\n",
        "        \n",
        "    def generate_user_story(self, visitor_id, overall_metrics_dict=None, overall_visitor_averages=None):\n",
        "        \"\"\"Generates a textual 'story' describing an individual visitor's behavior.\"\"\"\n",
        "        visitor_profiles_results = self.generate_visitor_profiles()\n",
        "        visitor_profiles = visitor_profiles_results['visitor_profiles_df']\n",
        "        user_profile = visitor_profiles[visitor_profiles['visitor_id'] == visitor_id] \n",
        "            \n",
        "        if user_profile.empty:\n",
        "            return \"No user story generated for this visitor ID.\"\n",
        "            \n",
        "        user_profile = user_profile.iloc[0]\n",
        "        story_lines = [f\"## User Profile Report: Visitor ID {int(user_profile['visitor_id'])}\"]\n",
        "            \n",
        "        story_lines.append(f\"**Visitor Company**: {user_profile['company']}\")\n",
        "            \n",
        "        avg_depth_overall = overall_metrics_dict.get('avg_depth_results', {}).get('avg_depth_per_visit', None)\n",
        "        overall_bounce_rate = overall_metrics_dict.get('bounce_results', {}).get('overall_bounce_rate', None)\n",
        "        new_depth_overall = overall_metrics_dict.get('new_vs_returning_results', {}).get('new_depth_comparison', None)\n",
        "        returning_depth_overall = overall_metrics_dict.get('new_vs_returning_results', {}).get('returning_depth_comparison', None)\n",
        "        overall_avg_downloads = overall_visitor_averages.get('download_count', 0.0) if overall_visitor_averages else 0.0\n",
        "        overall_avg_ir_score = overall_visitor_averages.get('investor_interest_score', 0.0) if overall_visitor_averages else 0.0\n",
        "        overall_avg_content_breadth = overall_visitor_averages.get('content_breadth', 0.0) if overall_visitor_averages else 0.0\n",
        "            \n",
        "        if user_profile['is_repeat_visitor']:\n",
        "            story_lines.append(\"This individual is a **returning visitor** to our website.\")\n",
        "            if returning_depth_overall is not None:\n",
        "                if user_profile['avg_session_depth'] < returning_depth_overall:\n",
        "                    story_lines.append(f\"However, their average session depth of {user_profile['avg_session_depth']:.2f} pages is somewhat lower than the average for returning visitors ({returning_depth_overall:.2f} pages). This might suggest a very specific goal-oriented visit or a change in their typical engagement.\")\n",
        "                elif user_profile['avg_session_depth'] > returning_depth_overall:\n",
        "                    story_lines.append(f\"Their average session depth of {user_profile['avg_session_depth']:.2f} pages is higher than the average for returning visitors ({returning_depth_overall:.2f} pages), indicating strong ongoing engagement.\")\n",
        "        else:\n",
        "            story_lines.append(\"This individual is a **new visitor** to our website.\")\n",
        "            if new_depth_overall is not None:\n",
        "                if user_profile['avg_session_depth'] > new_depth_overall:\n",
        "                    story_lines.append(f\"Notably, their average session depth of {user_profile['avg_session_depth']:.2f} pages is significantly higher than the average for new visitors ({new_depth_overall:.2f} pages), suggesting strong initial interest.\")\n",
        "                elif user_profile['avg_session_depth'] < new_depth_overall and user_profile['avg_session_depth'] > 0:\n",
        "                    story_lines.append(f\"Their average session depth of {user_profile['avg_session_depth']:.2f} pages is lower than the average for new visitors ({new_depth_overall:.2f} pages), which aligns with typical new user behavior of quick exploration or high bounce.\")\n",
        "\n",
        "        story_lines.append(f\"They typically view an average of **{user_profile['avg_session_depth']:.2f} pages per session**, indicating their typical level of engagement within each visit.\")\n",
        "        if avg_depth_overall is not None:\n",
        "            if user_profile['avg_session_depth'] > avg_depth_overall:\n",
        "                story_lines.append(f\"This is **higher than the overall website average session depth** of {avg_depth_overall:.2f} pages.\")\n",
        "            elif user_profile['avg_session_depth'] < avg_depth_overall:\n",
        "                story_lines.append(f\"This is **lower than the overall website average session depth** of {avg_depth_overall:.2f} pages.\")\n",
        "\n",
        "        if user_profile['download_count'] > 0:\n",
        "            story_lines.append(f\"A key action for this user is downloading content, as they have initiated **{int(user_profile['download_count'])} downloads**.\")\n",
        "            if user_profile['frequent_downloader']:\n",
        "                story_lines.append(\"They are categorized as a **frequent downloader**, suggesting a high propensity for consuming downloadable resources.\")\n",
        "            if user_profile['download_count'] > overall_avg_downloads:\n",
        "                story_lines.append(f\"This is notably higher than the overall average of {overall_avg_downloads:.1f} downloads per visitor.\")\n",
        "            else:\n",
        "                story_lines.append(f\"This is in line with or lower than the overall average of {overall_avg_downloads:.1f} downloads per visitor.\")\n",
        "        else:\n",
        "            story_lines.append(\"They have not initiated any downloads during their sessions, which might indicate different information needs or engagement patterns.\")\n",
        "\n",
        "        if user_profile['top_visited_section'] != 'No_Section_Determined':\n",
        "            story_lines.append(f\"Their primary area of interest on the site is the **'{user_profile['top_visited_section']}' section**, as indicated by the highest proportion of their pageviews.\")\n",
        "\n",
        "        if user_profile['ir_only_visitor']:\n",
        "            story_lines.append(\"This user **primarily browses Investor Relations content only** (tagged as an 'IR-only visitor').\")\n",
        "        elif user_profile['investor_interest_score'] > 0:\n",
        "            story_lines.append(f\"They show a notable interest in Investor Relations, with **{int(user_profile['investor_interest_score'])} pageviews** in this section, indicating they are likely tracking company performance or news.\")\n",
        "            if user_profile['investor_interest_score'] > overall_avg_ir_score:\n",
        "                story_lines.append(f\"This is higher than the overall average IR interest score of {overall_avg_ir_score:.1f}.\")\n",
        "            else:\n",
        "                story_lines.append(f\"This is in line with or lower than the overall average IR interest score of {overall_avg_ir_score:.1f}.\")\n",
        "\n",
        "        if user_profile['esg_visitor']:\n",
        "            story_lines.append(\"They demonstrate a **strong interest in ESG-related content**.\")\n",
        "\n",
        "        if user_profile['deep_path_visitor']:\n",
        "            story_lines.append(\"This user typically engages in **deep browse paths**, exploring more than 5 pages per session, suggesting a thorough and investigative approach to content consumption.\")\n",
        "        elif user_profile['avg_session_depth'] <= 1.1 and user_profile['avg_session_depth'] > 0:\n",
        "            story_lines.append(\"Conversely, this user exhibits **high bounce rate behavior**, often leaving after viewing just one page, which could signal a quick search for specific information or a lack of immediate relevance.\")\n",
        "            if overall_bounce_rate is not None and user_profile['avg_session_depth'] == 1:\n",
        "                story_lines.append(f\"Their behavior contributes to the overall website bounce rate, which is {overall_bounce_rate:.2%}.\")\n",
        "                \n",
        "        user_visits_for_path = self.df[self.df['visitor_id'] == visitor_id].sort_values(['id_visit', 'timestamp'])\n",
        "        user_visits_cleaned = user_visits_for_path.dropna(subset=['section'])\n",
        "        user_visits_cleaned = user_visits_cleaned[user_visits_cleaned['section'].str.lower() != 'nan']\n",
        "            \n",
        "        if not user_visits_cleaned.empty:\n",
        "            user_visits_cleaned['path'] = user_visits_cleaned.groupby('id_visit')['section'].transform(lambda x: ' -> '.join(x.astype(str)))\n",
        "            unique_paths = user_visits_cleaned.drop_duplicates(subset='path')['path']\n",
        "            if not unique_paths.empty:\n",
        "                story_lines.append(\"\\n**Typical Session Paths Observed:**\")\n",
        "                story_lines.append(\"This visitor typically follows these paths to explore our website:\")\n",
        "                for i, path in enumerate(unique_paths.head(3)):\n",
        "                    story_lines.append(f\"- {path}\")\n",
        "            else:\n",
        "                story_lines.append(\"\\nNo distinct session paths found for this user within the dataset.\")\n",
        "        else:\n",
        "            story_lines.append(\"\\nPath data for this visitor is incomplete and cannot be analyzed.\")\n",
        "\n",
        "        user_story = \"\\n\".join(story_lines)\n",
        "        return user_story\n",
        "\n",
        "\n",
        "\n",
        "class PredictiveModeler:\n",
        "    def __init__(self, dataframe):\n",
        "        self.df = dataframe.copy()\n",
        "        self.model = None\n",
        "        self.features = []\n",
        "\n",
        "    def prepare_longitudinal_data(self):\n",
        "        \"\"\"\n",
        "        Engineers time-series features and prepares the dataset for a predictive model.\n",
        "        This method predicts a 'future_download_probability' based on past behavior.\n",
        "        \"\"\"\n",
        "        df = self.df.sort_values(['visitor_id', 'timestamp'])\n",
        "        \n",
        "        # Define the target variable: a download in the next 30 days\n",
        "        df['future_download_within_30d'] = df.groupby('visitor_id')['download_flag'].transform(\n",
        "            lambda x: x.shift(-1).rolling(window=30, min_periods=1).apply(\n",
        "                lambda y: y.any(), raw=True).fillna(0).astype(int)\n",
        "        )\n",
        "        \n",
        "        # Feature Engineering: Lagged variables and temporal markers\n",
        "        df['lag_session_depth'] = df.groupby('visitor_id').cumcount().shift(1).fillna(0)\n",
        "        \n",
        "        df['lag_download_count'] = df.groupby('visitor_id')['download_flag'].cumsum().shift(1).fillna(0)\n",
        "        \n",
        "        # Temporal markers (e.g., day of week, hour of day)\n",
        "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "        df['hour_of_day'] = df['timestamp'].dt.hour\n",
        "        \n",
        "        # Event dummies (Requires event data to be loaded)\n",
        "        # This part is complex. We'll simplify for this example.\n",
        "        # Assuming you have a list of event dates, you can create a feature like:\n",
        "        # events_df = pd.read_csv('combined_company_events.csv')\n",
        "        # events_df['event_date'] = pd.to_datetime(events_df['event_date'])\n",
        "        # for date in events_df['event_date']:\n",
        "        #     df[f'event_proximity_{date.strftime(\"%Y%m%d\")}'] = (df['timestamp'] - date).dt.days.abs()\n",
        "        \n",
        "        # Select features for the model\n",
        "        self.features = ['lag_session_depth', 'lag_download_count', 'day_of_week', 'hour_of_day']\n",
        "        \n",
        "        # Filter for rows where we have a target variable\n",
        "        df_final = df.dropna(subset=['future_download_within_30d']).copy()\n",
        "        \n",
        "        # One-hot encode categorical features if any\n",
        "        df_final = pd.get_dummies(df_final, columns=['day_of_week', 'hour_of_day'], drop_first=True)\n",
        "        self.features = [col for col in df_final.columns if col in self.features or re.match(r'day_of_week_|hour_of_day_', col)]\n",
        "        \n",
        "        # Only use sessions that are not the first session for a visitor, to avoid data leakage\n",
        "        df_final = df_final[df_final['is_new_visitor_session'] == False]\n",
        "\n",
        "        return df_final\n",
        "    \n",
        "    def train_and_evaluate_gbm(self, df_longitudinal):\n",
        "        \"\"\"\n",
        "        Trains a Gradient Boosting Machine and evaluates its performance using\n",
        "        time-series cross-validation.\n",
        "        \"\"\"\n",
        "        if df_longitudinal.empty or len(self.features) == 0:\n",
        "            return None, \"Not enough data or features to train the model.\"\n",
        "\n",
        "        X = df_longitudinal[self.features]\n",
        "        y = df_longitudinal['future_download_within_30d']\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        \n",
        "        all_maes = []\n",
        "        all_aucs = []\n",
        "        \n",
        "        final_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "        for train_index, test_index in tscv.split(X):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "            \n",
        "            # Check if training or testing data is empty, skip if so.\n",
        "            if X_train.empty or X_test.empty:\n",
        "                continue\n",
        "                \n",
        "            final_model.fit(X_train, y_train)\n",
        "            \n",
        "            y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "            y_pred = final_model.predict(X_test)\n",
        "            \n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            auc = roc_auc_score(y_test, y_pred_proba)\n",
        "            \n",
        "            all_maes.append(mae)\n",
        "            all_aucs.append(auc)\n",
        "\n",
        "        if not all_maes: # Check if any folds were successfully run\n",
        "            return None, \"Training failed, no valid cross-validation folds.\"\n",
        "            \n",
        "        self.model = final_model\n",
        "        \n",
        "        metrics = {\n",
        "            'mean_mae': np.mean(all_maes),\n",
        "            'mean_auc': np.mean(all_aucs),\n",
        "            'last_trained_date': df_longitudinal['timestamp'].max()\n",
        "        }\n",
        "        \n",
        "        return self.model, metrics\n",
        "        \n",
        "    def predict(self, new_data):\n",
        "        \"\"\"\n",
        "        Makes a prediction using the trained model.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return \"Model not trained yet.\"\n",
        "        \n",
        "        # new_data needs to have the same features as the training data\n",
        "        new_data_processed = pd.get_dummies(new_data, columns=['day_of_week', 'hour_of_day'], drop_first=True)\n",
        "        missing_cols = set(self.features) - set(new_data_processed.columns)\n",
        "        for c in missing_cols:\n",
        "            new_data_processed[c] = 0\n",
        "        new_data_processed = new_data_processed[self.features]\n",
        "        \n",
        "        return self.model.predict_proba(new_data_processed)[:, 1]\n",
        "\n",
        "def generate_event_ai_analysis(df_event_impact_summary):\n",
        "    if df_event_impact_summary.empty:\n",
        "        return \"No event impact data available to generate insights.\"\n",
        "\n",
        "    analysis_lines = []\n",
        "    analysis_lines.append(\"## AI Summary: Company A Event Impact Analysis\")\n",
        "    analysis_lines.append(\"---\")\n",
        "    analysis_lines.append(\"This section analyzes how key financial and regulatory events for Company A correlate with changes in investor behavior on the website.\")\n",
        "\n",
        "    # Sort by Event Date for chronological analysis\n",
        "    df_event_impact_summary_sorted = df_event_impact_summary.sort_values(by='Event Date', ascending=True)\n",
        "\n",
        "    # General observations\n",
        "    significant_impact_events = df_event_impact_summary_sorted[\n",
        "        (df_event_impact_summary_sorted['Change in Visits (%)'].abs() > 10) |\n",
        "        (df_event_impact_summary_sorted['Change in Downloads (%)'].abs() > 10) |\n",
        "        (df_event_impact_summary_sorted['Change in Session Depth (%)'].abs() > 10)\n",
        "    ]\n",
        "\n",
        "    if not significant_impact_events.empty:\n",
        "        analysis_lines.append(\"\\n### Key Observations on Event Impact:\")\n",
        "        analysis_lines.append(\"We observed several events with notable shifts in investor behavior:\")\n",
        "        for index, event in significant_impact_events.iterrows():\n",
        "            event_date = event['Event Date'].strftime('%Y-%m-%d')\n",
        "            event_title = event['Event Title']\n",
        "            changes = []\n",
        "            if pd.notna(event['Change in Visits (%)']):\n",
        "                changes.append(f\"visits changed by **{event['Change in Visits (%)']:.2f}%**\")\n",
        "            if pd.notna(event['Change in Downloads (%)']):\n",
        "                changes.append(f\"downloads changed by **{event['Change in Downloads (%)']:.2f}%**\")\n",
        "            if pd.notna(event['Change in Session Depth (%)']):\n",
        "                changes.append(f\"session depth changed by **{event['Change in Session Depth (%)']:.2f}%**\")\n",
        "\n",
        "            if changes:\n",
        "                analysis_lines.append(f\"- **{event_title} ({event_date})**: {' and '.join(changes)}.\")\n",
        "            else:\n",
        "                analysis_lines.append(f\"- **{event_title} ({event_date})**: No significant quantifiable changes observed for this event.\")\n",
        "    else:\n",
        "        analysis_lines.append(\"\\n### General Observations:\")\n",
        "        analysis_lines.append(\"No single event triggered a dramatic shift (more than 10% change) in visitor behavior metrics during their respective windows.\")\n",
        "        analysis_lines.append(\"This could mean investor behavior is consistently engaged, or the events had a more subtle, prolonged impact not captured by direct window comparison.\")\n",
        "\n",
        "    # Identify top positive/negative impacts\n",
        "    most_impactful_visit_increase = df_event_impact_summary_sorted.nlargest(1, 'Change in Visits (%)', keep='first')\n",
        "    most_impactful_visit_decrease = df_event_impact_summary_sorted.nsmallest(1, 'Change in Visits (%)', keep='first')\n",
        "    \n",
        "    most_impactful_download_increase = df_event_impact_summary_sorted.nlargest(1, 'Change in Downloads (%)', keep='first')\n",
        "    most_impactful_download_decrease = df_event_impact_summary_sorted.nsmallest(1, 'Change in Downloads (%)', keep='first')\n",
        "\n",
        "    analysis_lines.append(\"\\n### Key Insights & Recommendations:\")\n",
        "\n",
        "    if not most_impactful_visit_increase.empty and most_impactful_visit_increase.iloc[0]['Change in Visits (%)'] > 0:\n",
        "        event = most_impactful_visit_increase.iloc[0]\n",
        "        analysis_lines.append(f\"- The event **'{event['Event Title']}'** on {event['Event Date'].strftime('%Y-%m-%d')} led to the highest increase in **Total Visits** ({event['Change in Visits (%)']:.2f}%). This indicates strong public or investor interest surrounding this type of announcement. **Recommendation:** Analyze the specific content and communication strategy for this event and replicate successful elements for similar future announcements.\")\n",
        "    \n",
        "    if not most_impactful_download_increase.empty and most_impactful_download_increase.iloc[0]['Change in Downloads (%)'] > 0:\n",
        "        event = most_impactful_download_increase.iloc[0]\n",
        "        analysis_lines.append(f\"- The event **'{event['Event Title']}'** on {event['Event Date'].strftime('%Y-%m-%d')} significantly boosted **Downloads** ({event['Change in Downloads (%)']:.2f}%). This suggests the associated materials (e.g., reports, presentations) were highly relevant or effectively promoted. **Recommendation:** Ensure all relevant documents for high-impact events are prominently featured and easily accessible. Consider pre-event teasers for upcoming reports.\")\n",
        "\n",
        "    if not most_impactful_visit_decrease.empty and most_impactful_visit_decrease.iloc[0]['Change in Visits (%)'] < 0:\n",
        "        event = most_impactful_visit_decrease.iloc[0]\n",
        "        analysis_lines.append(f\"- Conversely, the event **'{event['Event Title']}'** on {event['Event Date'].strftime('%Y-%m-%d')} saw the largest drop in **Total Visits** ({event['Change in Visits (%)']:.2f}%). This could indicate lower interest, saturation, or perhaps the event itself was less critical. **Recommendation:** Evaluate the type of information presented, its timing, and promotion for events that cause visitor declines to understand if adjustments are needed.\")\n",
        "\n",
        "    if not most_impactful_download_decrease.empty and most_impactful_download_decrease.iloc[0]['Change in Downloads (%)'] < 0:\n",
        "        event = most_impactful_download_decrease.iloc[0]\n",
        "        analysis_lines.append(f\"- **Downloads** decreased most around **'{event['Event Title']}'** on {event['Event Date'].strftime('%Y-%m-%d')} ({event['Change in Downloads (%)']:.2f}%). This suggests the associated documents may have been less appealing or harder to find. **Recommendation:** Review the content, relevance, and discoverability of materials released around low-engagement events.\")\n",
        "\n",
        "    # General recommendations based on all data\n",
        "    analysis_lines.append(\"\\n### General Strategic Recommendations:\")\n",
        "    analysis_lines.append(\"- **Proactive Communication**: Leverage the insights from events that generated positive spikes in engagement. Plan pre-event communication, ensure clear messaging, and easy access to new content as soon as it's released.\")\n",
        "    analysis_lines.append(\"- **Content Relevance**: Continuously assess if the information provided around events truly meets the needs of your investor audience. Use download figures and session depth as indicators of content value.\")\n",
        "    analysis_lines.append(\"- **Multi-Channel Promotion**: Events are opportunities for multiple touchpoints. Ensure the website content is integrated with email alerts, social media announcements, and news releases to maximize reach.\")\n",
        "    analysis_lines.append(\"- **Segmented Analysis**: As a next step, combining this event data with the **Visitor Profiles (Clusters)** would allow for a deeper understanding of *which specific investor segments* are most responsive to different types of events. This would enable highly targeted communication strategies.\")\n",
        "\n",
        "    return \"\\n\".join(analysis_lines)\n",
        "\n",
        "# ---  generate_website_ai_analysis ---\n",
        "def generate_website_ai_analysis(selected_website, metrics_filtered, global_benchmark_metrics, tag_explanations, combined_pageviews_for_ai, cluster_content_interest_df=None):\n",
        "    ai_analysis = []\n",
        "    if cluster_content_interest_df is not None and not cluster_content_interest_df.empty:\n",
        "        ai_analysis.append(\"\\n### Targeted Content & Audience Insights:\")\n",
        "\n",
        "        for cluster_name in cluster_content_interest_df['Cluster Name'].unique():\n",
        "            cluster_data = cluster_content_interest_df[cluster_content_interest_df['Cluster Name'] == cluster_name].sort_values('Interest Score', ascending=False)\n",
        "            if not cluster_data.empty:\n",
        "                top_section = cluster_data.iloc[0]['Section']\n",
        "                score = cluster_data.iloc[0]['Interest Score']\n",
        "                ai_analysis.append(f\"- **{cluster_name}**: shows highest interest in '{top_section}' (score: {score:.1f}).\")\n",
        "\n",
        "                if 'High-Intent' in cluster_name and 'Investor Relations' in top_section:\n",
        "                    ai_analysis.append(f\"  **Action**: Continue enriching {top_section} with detailed analytics and reports for these key users.\")\n",
        "                elif 'New Visitors' in cluster_name and 'Product' in top_section:\n",
        "                    ai_analysis.append(f\"  **Action**: Optimize the onboarding experience and clear CTAs on {top_section} to convert new visitors.\")\n",
        "    else:\n",
        "        ai_analysis.append(\"\\n### Targeted Content & Audience Insights: (No clustered content interest data for deeper insights)\")\n",
        "    # 1. Overall Performance Summary and Comparison\n",
        "    current_avg_depth = metrics_filtered.get('avg_depth', 0.0)\n",
        "    current_bounce_rate = metrics_filtered.get('bounce_rate', 0.0)\n",
        "    current_total_downloads = metrics_filtered.get('total_downloads', 0)\n",
        "    current_total_visits = metrics_filtered.get('total_visits', 0)\n",
        "    current_downloads_per_visit = current_total_downloads / current_total_visits if current_total_visits > 0 else 0.0\n",
        "\n",
        "    global_avg_depth = global_benchmark_metrics.get('avg_depth', 0.0)\n",
        "    global_bounce_rate = global_benchmark_metrics.get('bounce_rate', 0.0)\n",
        "    global_avg_downloads_per_visit = global_benchmark_metrics.get('avg_downloads_per_visit', 0.0)\n",
        "\n",
        "    ai_analysis.append(f\"## AI Analysis for {selected_website} Website\")\n",
        "    ai_analysis.append(\"---\")\n",
        "    ai_analysis.append(\"### Performance Overview:\")\n",
        "    ai_analysis.append(f\"The website for **{selected_website}** currently records **{current_total_visits} visits**, with an average session depth of **{current_avg_depth:.2f} pages** and a bounce rate of **{current_bounce_rate:.2%}**.\")\n",
        "    ai_analysis.append(f\"Users initiated **{current_total_downloads} downloads** on this site.\")\n",
        "\n",
        "    ai_analysis.append(\"\\n### Key Comparisons:\")\n",
        "    if current_avg_depth > global_avg_depth:\n",
        "        ai_analysis.append(f\"- **Higher Session Depth ({current_avg_depth:.2f} vs Global Avg: {global_avg_depth:.2f})**: This indicates that visitors to {selected_website} are **more engaged** and explore deeper into the site than the overall average. This is a positive sign, suggesting relevant content or good internal linking.\")\n",
        "    else:\n",
        "        ai_analysis.append(f\"- **Lower Session Depth ({current_avg_depth:.2f} vs Global Avg: {global_avg_depth:.2f})**: Visitors to {selected_website} tend to explore less deeply. Consider optimizing content layout, calls-to-action, and internal linking to encourage further exploration.\")\n",
        "\n",
        "    if current_bounce_rate < global_bounce_rate:\n",
        "        ai_analysis.append(f\"- **Lower Bounce Rate ({current_bounce_rate:.2%} vs Global Avg: {global_bounce_rate:.2%})**: This is excellent! It suggests landing pages are highly effective, retaining visitors and encouraging them to proceed beyond the entry page.\")\n",
        "    else:\n",
        "        ai_analysis.append(f\"- **Higher Bounce Rate ({current_bounce_rate:.2%} vs Global Avg: {global_bounce_rate:.2%})**: A high bounce rate indicates that initial landing pages might not be meeting visitor expectations or engagement is low. Focus on improving first-page experience and clarity.\")\n",
        "    \n",
        "    if current_downloads_per_visit > global_avg_downloads_per_visit:\n",
        "        ai_analysis.append(f\"- **Higher Download-per-Visit Ratio ({current_downloads_per_visit:.2f} vs Global Avg: {global_avg_downloads_per_visit:.2f})**: This website is highly effective at converting visits into tangible interest (downloads). Highlight key resources and ensure smooth download processes.\")\n",
        "    else:\n",
        "        ai_analysis.append(f\"- **Lower Download-per-Visit Ratio ({current_downloads_per_visit:.2f} vs Global Avg: {global_avg_downloads_per_visit:.2f})**: Consider improving the visibility, accessibility, or perceived value of downloadable content to boost conversion rates.\")\n",
        "\n",
        "    # 2. Content & Audience Insights\n",
        "    ai_analysis.append(\"\\n### Content and Audience Insights:\")\n",
        "    top_pageview_section = metrics_filtered['pageview'].index[0] if not metrics_filtered['pageview'].empty else 'N/A'\n",
        "    if top_pageview_section != 'N/A':\n",
        "        ai_analysis.append(f\"- The most popular content area is **'{top_pageview_section}'**, indicating strong visitor interest here. Continue to invest in and promote content within this section.\")\n",
        "        \n",
        "        # Compare pageview percentage for top section with overall\n",
        "        website_top_section_pct = combined_pageviews_for_ai[(combined_pageviews_for_ai['section'] == top_pageview_section) & (combined_pageviews_for_ai['type'] == selected_website)]['percentage'].iloc[0] if not combined_pageviews_for_ai.empty and not combined_pageviews_for_ai[(combined_pageviews_for_ai['section'] == top_pageview_section) & (combined_pageviews_for_ai['type'] == selected_website)].empty else 0\n",
        "        overall_top_section_pct = combined_pageviews_for_ai[(combined_pageviews_for_ai['section'] == top_pageview_section) & (combined_pageviews_for_ai['type'] == 'Overall')]['percentage'].iloc[0] if not combined_pageviews_for_ai.empty and not combined_pageviews_for_ai[(combined_pageviews_for_ai['section'] == top_pageview_section) & (combined_pageviews_for_ai['type'] == 'Overall')].empty else 0\n",
        "\n",
        "        if website_top_section_pct > overall_top_section_pct:\n",
        "            ai_analysis.append(f\"  - This section is disproportionately popular ({website_top_section_pct:.2f}%) compared to its overall website average ({overall_top_section_pct:.2f}%), highlighting a unique strength or focus of {selected_website}.\")\n",
        "\n",
        "    if not metrics_filtered['tag_counts'].empty:\n",
        "        top_tag = metrics_filtered['tag_counts'].index[0]\n",
        "        ai_analysis.append(f\"- A dominant segment of visitors are categorized as **'{top_tag}'**. This group likely drives significant interactions. ({tag_explanations.get(top_tag, 'No explanation available.').split('.')[0].strip()}).\")\n",
        "        \n",
        "        # Suggest actions based on top tag\n",
        "        if 'High-Intent' in top_tag:\n",
        "            ai_analysis.append(\"  - **Action**: Prioritize personalized outreach or direct engagement strategies for these high-value users.\")\n",
        "        elif 'High Bounce Rate' in top_tag:\n",
        "            ai_analysis.append(\"  - **Action**: Rework landing pages and initial content to better capture the attention of this segment and reduce immediate exits.\")\n",
        "        elif 'IR-Only Browser' in top_tag:\n",
        "            ai_analysis.append(\"  - **Action**: Ensure IR content is easily accessible and consider cross-promoting related non-IR content subtly to broaden their engagement.\")\n",
        "    \n",
        "    # 3. Actionable Recommendations\n",
        "    ai_analysis.append(\"\\n### Actionable Recommendations:\")\n",
        "    ai_analysis.append(\"- **Content Strategy**: Given the popular sections and visitor tags, consider developing more in-depth content or interactive tools within high-interest areas.\")\n",
        "    ai_analysis.append(\"- **User Experience**: Analyze the paths of high-bounce visitors. Are there broken links, slow loading times, or confusing navigation preventing deeper engagement?\")\n",
        "    ai_analysis.append(\"- **Conversion Optimization**: If download rates are lower than desired, A/B test different call-to-action placements, button designs, or resource descriptions to improve conversions.\")\n",
        "    ai_analysis.append(\"- **Personalization**: For repeat visitors or specific clusters, explore personalized content recommendations or targeted messaging to enhance their experience.\")\n",
        "    if current_bounce_rate > global_bounce_rate and not metrics_filtered['tag_counts'].empty and 'New Visitor' in metrics_filtered['tag_counts'].index[0]:\n",
        "        ai_analysis.append(\"- **Targeted UX Improvement**: Given high overall bounce rate and a large 'New Visitor' segment, prioritize A/B testing of landing page designs and content clarity to immediately engage new users.\")\n",
        "    \n",
        "    # Example: If download rate is low, but 'Frequent Downloader' tag exists\n",
        "    if current_downloads_per_visit < global_avg_downloads_per_visit and not metrics_filtered['tag_counts'].empty and 'Frequent Downloader' in metrics_filtered['tag_counts'].index:\n",
        "        ai_analysis.append(\"- **Content Accessibility**: Despite having frequent downloaders, the overall download ratio is low. This suggests a discoverability issue. Ensure top downloadable resources are prominently linked and easily navigable, especially for repeat visitors.\")\n",
        "    return \"\\n\".join(ai_analysis)\n",
        "\n",
        "# --- NEW HELPER FUNCTION: Full Cluster Descriptions for Modal/Bottom Section ---\n",
        "def generate_cluster_full_description_content():\n",
        "    content = []\n",
        "    content.append(html.P(\"Here are the detailed profiles for each visitor segment identified by our clustering analysis. These personas can help you tailor content and engagement strategies.\", className=\"mb-3\"))\n",
        "    for persona in cluster_personas_data:\n",
        "        content.append(html.H5(f\"Cluster {persona['num']}: {persona['name']}\", className=\"mt-4\"))\n",
        "        content.append(dcc.Markdown(persona['full']))\n",
        "        content.append(html.Hr())\n",
        "    return html.Div(content)\n",
        "\n",
        "\n",
        "# --- Dashboard Data Preparation (Keep as is) ---\n",
        "def prepare_dashboard_data(file_path, sitemap_file_path):\n",
        "    \"\"\"Loads, preprocesses data, and calculates all initial metrics for the dashboards.\"\"\"\n",
        "    print(\"--- Preparing Dashboard Data ---\")\n",
        "    data_loader = DataLoader(file_path)\n",
        "    df_all = data_loader.load_and_preprocess_data()\n",
        "    \n",
        "    if df_all.empty:\n",
        "        print(\"Error: Processed DataFrame is empty. Cannot proceed with analysis.\")\n",
        "\n",
        "        return (pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), {}, {}, {}, \n",
        "                pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()) # Added two new empty DFs for clustered/overall behavior\n",
        "\n",
        "\n",
        "    analyzer = BehaviorAnalyzer(df_all)\n",
        "    \n",
        "\n",
        "    pageview_metrics = analyzer.calculate_pageview_metrics()\n",
        "    bounce_results = analyzer.calculate_bounce_rate()\n",
        "    download_results = analyzer.analyze_download_behavior()\n",
        "    funnel_results = analyzer.perform_funnel_analysis()\n",
        "    avg_depth_results = analyzer.calculate_average_depth()\n",
        "    unique_investors_results = analyzer.analyze_unique_investors()\n",
        "    session_path_results = analyzer.analyze_session_paths()\n",
        "    time_distribution_results = analyzer.analyze_time_distribution()\n",
        "    session_depth_repeat_results = analyzer.analyze_session_path_length_and_repeat_visitors()\n",
        "    new_vs_returning_results = analyzer.compare_new_vs_returning()\n",
        "    country_comparison_results = analyzer.compare_by_country()\n",
        "    common_paths_results = analyzer.analyze_most_common_paths()\n",
        "    \n",
        "    visitor_profiles_results = analyzer.generate_visitor_profiles()\n",
        "    df_profiles = visitor_profiles_results['visitor_profiles_df'] \n",
        "    \n",
        "    clustering_results = analyzer.perform_clustering_and_profiling(n_clusters=5) \n",
        "    if 'cluster' in clustering_results.get('visitor_profiles_with_clusters', pd.DataFrame()).columns:\n",
        "        df_profiles = clustering_results['visitor_profiles_with_clusters'] \n",
        "\n",
        "    if 'gmm_comparison' in clustering_results.get('metrics_data', {}):\n",
        "        gmm_data = clustering_results['metrics_data']['gmm_comparison']\n",
        "        print(\"\\n--- GMM and BIC Comparison Results ---\")\n",
        "        if pd.notna(gmm_data.get('best_gmm_n_components')):\n",
        "            print(f\"Best GMM model has {int(gmm_data['best_gmm_n_components'])} components with a BIC score of {gmm_data['best_gmm_bic']:.2f}\")\n",
        "        else:\n",
        "            print(\"GMM comparison could not be performed or did not find a valid model.\")\n",
        "\n",
        "    if not df_profiles.empty:\n",
        "        df_all_with_profiles = df_all.merge(df_profiles[['visitor_id', 'cluster', 'user_tags']], on='visitor_id', how='left')\n",
        "        df_all_with_profiles['cluster'] = df_all_with_profiles['cluster'].fillna(-1).astype(int) # -1 for unclustered\n",
        "    else:\n",
        "        df_all_with_profiles = df_all.copy()\n",
        "        df_all_with_profiles['cluster'] = -1 # Default cluster for all if no profiles\n",
        "        df_all_with_profiles['user_tags'] = '' # Default empty tags\n",
        "\n",
        "    if not df_profiles.empty:\n",
        "        df_all_with_profiles = df_all.merge(df_profiles[['visitor_id', 'cluster', 'user_tags']], on='visitor_id', how='left')\n",
        "        df_all_with_profiles['cluster'] = df_all_with_profiles['cluster'].fillna(-1).astype(int) # -1 for unclustered\n",
        "    else:\n",
        "        \n",
        "        df_all_with_profiles = df_all.copy()\n",
        "        df_all_with_profiles['cluster'] = -1 # Default cluster for all if no profiles\n",
        "        df_all_with_profiles['user_tags'] = '' # Default empty tags\n",
        "\n",
        "\n",
        "  \n",
        "    df_events = pd.DataFrame()\n",
        "    df_event_impact_summary = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        df_events = pd.read_csv('combined_company_events.csv')\n",
        "        df_events['event_date'] = pd.to_datetime(df_events['event_date'], errors='coerce')\n",
        "        df_events.dropna(subset=['event_date'], inplace=True)\n",
        "        df_events = df_events[df_events['company'] == 'Vodafone'].copy()\n",
        "        print(f\"Loaded {len(df_events)} Vodafone events.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: 'combined_company_events.csv' not found. Event data will not be available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading combined_company_events.csv: {e}\")\n",
        "\n",
        "    try:\n",
        "        df_event_impact_summary = pd.read_csv('company_a_event_impact_summary.csv')\n",
        "        df_event_impact_summary['Event Date'] = pd.to_datetime(df_event_impact_summary['Event Date'], errors='coerce')\n",
        "        df_event_impact_summary.dropna(subset=['Event Date'], inplace=True)\n",
        "        print(f\"Loaded {len(df_event_impact_summary)} event impact summaries.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: 'company_a_event_impact_summary.csv' not found. Event impact summary will not be available.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading company_a_event_impact_summary.csv: {e}\")\n",
        "\n",
        "\n",
        "    df_daily_company_a_behavior_clustered = pd.DataFrame() # Initialize\n",
        "    df_daily_company_a_behavior_overall = pd.DataFrame() # Initialize\n",
        "\n",
        "    if not df_all_with_profiles.empty: \n",
        "        df_company_a_behavior_raw_with_profiles = df_all_with_profiles[df_all_with_profiles['website_company'] == 'Company A'].copy()\n",
        "        if not df_company_a_behavior_raw_with_profiles.empty:\n",
        "            df_daily_company_a_behavior_clustered = df_company_a_behavior_raw_with_profiles.groupby([\n",
        "                df_company_a_behavior_raw_with_profiles['timestamp'].dt.date, 'cluster'\n",
        "            ]).agg(\n",
        "                total_visits=('id_visit', 'nunique'),\n",
        "                total_pageviews=('id_visit', 'count'),\n",
        "                total_downloads=('download_flag', lambda x: x.sum() if pd.api.types.is_bool_dtype(x) else (x == True).sum()),\n",
        "                avg_session_depth=('id_visit', lambda x: x.count() / x.nunique() if x.nunique() > 0 else 0)\n",
        "            ).reset_index()\n",
        "            df_daily_company_a_behavior_clustered.rename(columns={'timestamp': 'date'}, inplace=True)\n",
        "            df_daily_company_a_behavior_clustered['date'] = pd.to_datetime(df_daily_company_a_behavior_clustered['date'])\n",
        "            print(f\"Prepared daily Company A behavior data with clusters: {len(df_daily_company_a_behavior_clustered)} rows.\")\n",
        "            \n",
        "            df_daily_company_a_behavior_overall = df_company_a_behavior_raw_with_profiles.groupby(df_company_a_behavior_raw_with_profiles['timestamp'].dt.date).agg(\n",
        "                total_visits=('id_visit', 'nunique'),\n",
        "                total_pageviews=('id_visit', 'count'),\n",
        "                total_downloads=('download_flag', lambda x: x.sum() if pd.api.types.is_bool_dtype(x) else (x == True).sum()),\n",
        "                avg_session_depth=('id_visit', lambda x: x.count() / x.nunique() if x.nunique() > 0 else 0)\n",
        "            ).reset_index()\n",
        "            df_daily_company_a_behavior_overall.rename(columns={'timestamp': 'date'}, inplace=True)\n",
        "            df_daily_company_a_behavior_overall['date'] = pd.to_datetime(df_daily_company_a_behavior_overall['date'])\n",
        "            \n",
        "        else:\n",
        "            print(\"No Company A behavior data found in df_all_with_profiles for daily aggregation.\")\n",
        "            df_daily_company_a_behavior_overall = pd.DataFrame(columns=['date', 'total_visits', 'total_pageviews', 'total_downloads', 'avg_session_depth'])\n",
        "            df_daily_company_a_behavior_clustered = pd.DataFrame(columns=['date', 'cluster', 'total_visits', 'total_pageviews', 'total_downloads', 'avg_session_depth'])\n",
        "    else:\n",
        "        df_daily_company_a_behavior_overall = pd.DataFrame(columns=['date', 'total_visits', 'total_pageviews', 'total_downloads', 'avg_session_depth'])\n",
        "        df_daily_vodafone_behavior_clustered = pd.DataFrame(columns=['date', 'cluster', 'total_visits', 'total_pageviews', 'total_downloads', 'avg_session_depth'])\n",
        "\n",
        "\n",
        "    # Consolidate all detailed metrics into a single dictionary\n",
        "    detailed_metrics = {\n",
        "        'pageview_metrics': pageview_metrics['metrics_data'],\n",
        "        'bounce_results': bounce_results['metrics_data'],\n",
        "        'download_results': download_results['metrics_data'],\n",
        "        'funnel_results': funnel_results['metrics_data'],\n",
        "        'avg_depth_results': avg_depth_results['metrics_data'],\n",
        "        'unique_investors_results': unique_investors_results['metrics_data'],\n",
        "        'session_path_results': session_path_results['metrics_data'],\n",
        "        'time_distribution_results': time_distribution_results['metrics_data'],\n",
        "        'session_depth_repeat_results': session_depth_repeat_results['metrics_data'],\n",
        "        'new_vs_returning_results': new_vs_returning_results['metrics_data'],\n",
        "        'country_comparison_results': country_comparison_results['metrics_data'],\n",
        "        'common_paths_results': common_paths_results['metrics_data'],\n",
        "        'visitor_profiles_data': visitor_profiles_results['metrics_data'],\n",
        "        'clustering_results': clustering_results['metrics_data'],\n",
        "    }\n",
        "    \n",
        "    cluster_descriptions = clustering_results.get('cluster_descriptions', {})\n",
        "    \n",
        "    overall_visitor_averages = {}\n",
        "    if not df_profiles.empty:\n",
        "        relevant_profile_features = [f for f in BehaviorAnalyzer.CLUSTERING_FEATURES if f in df_profiles.columns]\n",
        "        overall_visitor_averages = df_profiles[relevant_profile_features].mean().to_dict()\n",
        "    else:\n",
        "        overall_visitor_averages = {feature: 0.0 for feature in BehaviorAnalyzer.CLUSTERING_FEATURES}\n",
        "            \n",
        "    sitemap_df = pd.DataFrame()\n",
        "    try:\n",
        "        with open(sitemap_file_path, 'r') as f:\n",
        "            sitemap_data = json.load(f)\n",
        "        sitemap_list = []\n",
        "        for section, subsections in sitemap_data.items():\n",
        "            for subsection in subsections:\n",
        "                sitemap_list.append({'section': section, 'subsection': subsection})\n",
        "        sitemap_df = pd.DataFrame(sitemap_list)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Sitemap file not found at {sitemap_file_path}. Sitemap visualization will be based on raw data.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sitemap JSON: {e}\")\n",
        "        sitemap_df = pd.DataFrame()\n",
        "\n",
        "    print(\"--- Dashboard Data Preparation Complete ---\")\n",
        "    return (df_all, df_all_with_profiles, df_profiles, detailed_metrics, cluster_descriptions, \n",
        "            overall_visitor_averages, sitemap_df, df_events, df_event_impact_summary, \n",
        "            df_daily_vodafone_behavior_clustered, df_daily_vodafone_behavior_overall)\n",
        "\n",
        "    \n",
        "\n",
        "def open_browser_after_startup():\n",
        "    \"\"\"Opens the Dash app in the default web browser after a short delay.\"\"\"\n",
        "    time.sleep(1.5) # Give the server a moment to start\n",
        "    url = \"http://127.0.0.1:8050/\"\n",
        "    try:\n",
        "        webbrowser.open_new_tab(url)\n",
        "        print(f\"Dash app has been opened in the browser: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unable to open browser automatically, please visit manually: {url}\")\n",
        "        print(f\"Error message: {e}\")\n",
        "\n",
        "# --- Main Program Logic and Dash App Initialization ---\n",
        "\n",
        "data_file_path = 'all_traffic_data_merged.csv'\n",
        "sitemap_framework_path = 'universal_website_category_framework.json'\n",
        "\n",
        "# Prepare all data and metrics once at startup\n",
        "df_processed, df_all_with_profiles, df_profiles, overall_metrics, cluster_descriptions, overall_visitor_averages, sitemap_df, df_events, df_event_impact_summary, df_daily_vodafone_behavior_clustered, df_daily_vodafone_behavior_overall = \\\n",
        "    prepare_dashboard_data(data_file_path, sitemap_framework_path)\n",
        "\n",
        "# Extract unique companies and website companies for dropdowns\n",
        "companies = sorted(df_processed['company'].unique().tolist()) if 'company' in df_processed.columns and not df_processed.empty else ['All Companies']\n",
        "website_companies = sorted(df_processed['website_company'].unique().tolist()) if 'website_company' in df_processed.columns and not df_processed.empty else ['All Websites']\n",
        "\n",
        "# Get all unique user tags for display and tooltips\n",
        "all_tags = set(tag.strip() for tags_str in df_profiles['user_tags'].unique() for tag in tags_str.split(',') if tag) if not df_profiles.empty else set()\n",
        "\n",
        "# Explanations for user tags (translated to English for this response)\n",
        "tag_explanations = {\n",
        "    'IR-Only Browser': 'Only visits Investor Relations (IR) content, indicating a focus on company financials and strategy.',\n",
        "    'Frequent Downloader': 'Downloads multiple files, showing a high demand for downloadable resources like reports and whitepapers.',\n",
        "    'Deep Path Visitor': 'Average session depth is over 5 pages, indicating in-depth research and high engagement.',\n",
        "    'ESG-Focused': 'Visited ESG (Environmental, Social, and Governance) related pages, showing interest in sustainable investing.',\n",
        "    'High Bounce Rate': 'Average session depth is close to 1, often leaving after visiting just one page.',\n",
        "    'High-Intent': 'Exhibits both download behavior and IR content visits, suggesting a high-intent investor or partner.',\n",
        "    'Repeat Visitor': 'Visited the website multiple times, indicating continued interest or a specific need.',\n",
        "    'New Visitor': 'This is their first recorded session on the website.' # Added explanation for New Visitor\n",
        "}\n",
        "\n",
        "# ---  Cluster Glossary/Personas ---\n",
        "# Pre-process cluster persona data for display (now includes cluster number in name)\n",
        "cluster_personas_data = []\n",
        "for cluster_id_str, description in cluster_descriptions.items():\n",
        "    # cluster_id_str is like \"Cluster 0\", \"Cluster 1\"\n",
        "    cluster_num = int(cluster_id_str.split(' ')[1])\n",
        "    \n",
        "    # Extract the concise persona name (e.g., \"Highly Engaged Core Investors\")\n",
        "    match = re.match(r'\\*\\*(.*?):\\*\\*', description)\n",
        "    persona_name = match.group(1).strip() if match else f\"Cluster {cluster_num}\"\n",
        "    \n",
        "    # Take the first sentence or a very short summary for the brief overview\n",
        "    brief_description = description.split('.')[0] + '.' if '.' in description else description\n",
        "    \n",
        "    cluster_personas_data.append({\n",
        "        'id': cluster_id_str, # e.g., \"Cluster 0\"\n",
        "        'num': cluster_num,    # e.g., 0\n",
        "        'name': persona_name,\n",
        "        'brief': brief_description,\n",
        "        'full': description\n",
        "    })\n",
        "# Sort by cluster number to ensure consistent order in display\n",
        "cluster_personas_data = sorted(cluster_personas_data, key=lambda x: x['num'])\n",
        "\n",
        "\n",
        "# Dash App Instance\n",
        "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.CERULEAN])\n",
        "\n",
        "# --- Optimized App Layout ---\n",
        "# Crucial change: Buttons that are 'Input' in callbacks must be STATICALLY in the layout.\n",
        "# Their visibility can be controlled by another callback.\n",
        "app.layout = dbc.Container([\n",
        "    dbc.Row(dbc.Col(html.H1(\"Investor Behavior Intelligence\", className=\"text-center text-primary my-4\"))),\n",
        "    html.Hr(className=\"my-2\"),\n",
        "\n",
        "    # === Static Buttons for Modals (Controlled by visibility callbacks) ===\n",
        "    # These buttons must be in the initial layout for callbacks to find them.\n",
        "    # We will control their 'display' style based on the active tab.\n",
        "    # Placed globally so they are always in the DOM for callbacks.\n",
        "    html.Div([\n",
        "        dbc.Button(\n",
        "            \"Tag Glossary (Detailed Explanations)\",\n",
        "            id=\"open-tag-glossary-modal-website\",\n",
        "            size=\"sm\",\n",
        "            className=\"mb-3\",\n",
        "            style={'display': 'none'} # Initially hidden, visibility controlled by callback\n",
        "        ),\n",
        "        dbc.Button(\n",
        "            \"What are these clusters? (Detailed Glossary)\",\n",
        "            id=\"open-cluster-glossary-modal-website-tab\",\n",
        "            size=\"sm\",\n",
        "            className=\"mb-3 ms-2\", # Add margin for separation\n",
        "            style={'display': 'none'} # Initially hidden, visibility controlled by callback\n",
        "        ),\n",
        "        dbc.Button(\n",
        "            \"View Detailed Personas\", # For visitor tab's cluster overview\n",
        "            id=\"open-cluster-glossary-modal-visitor-tab\",\n",
        "            size=\"sm\",\n",
        "            className=\"mb-3 ms-2\", # Add margin for separation\n",
        "            style={'display': 'none'} # Initially hidden, visibility controlled by callback\n",
        "        ),\n",
        "    ], className=\"text-end mb-4\"), # Container for global buttons, aligned right\n",
        "\n",
        "\n",
        "    dbc.Tabs(id=\"tabs-main\", active_tab=\"tab-website-company\", children=[\n",
        "        # --- Website Company-Level Dashboard ---\n",
        "        dbc.Tab(label=\"Website Company Insights\", tab_id=\"tab-website-company\", children=[\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.Div(id='website-executive-summary-container'), md=8),\n",
        "                dbc.Col(dcc.Dropdown(\n",
        "                    id='website-company-dropdown',\n",
        "                    options=[{'label': 'All Websites', 'value': 'All'}] + [{'label': wc, 'value': wc} for wc in website_companies],\n",
        "                    value='All',\n",
        "                    clearable=False,\n",
        "                    className=\"mt-4\"\n",
        "                ), md=4)\n",
        "            ], align=\"center\"),\n",
        "\n",
        "            # website-company-dashboard-content will be the MAIN output for this tab's dynamic content\n",
        "            html.Div(id='website-company-dashboard-content'),\n",
        "\n",
        "            # This div will contain the DataTable for Tag Behavior, no button here directly\n",
        "            # The button for tag glossary is now global (id=\"open-tag-glossary-modal-website\")\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.Hr(className=\"my-4\")),\n",
        "                dbc.Col(html.H3(\"Visitor Tag Group Behavior Comparison\", className=\"mb-3\")),\n",
        "                dbc.Col(html.Div(id='tag-behavior-table-container'), width=12),\n",
        "            ], className=\"mt-5 mb-4\"),\n",
        "\n",
        "            # Cluster Persona Description is now only via modal, triggered by global button\n",
        "            # Removed the direct display container for cluster description here.\n",
        "        ]),\n",
        "\n",
        "        # --- Visitor Company-Level Dashboard ---\n",
        "        dbc.Tab(label=\"Visitor Company Insights\", tab_id=\"tab-visitor-company\", children=[\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.H2(\"Overall Website Performance & Visitor Profiling\", className=\"my-4\")),\n",
        "                dbc.Col(dcc.Dropdown(\n",
        "                    id='visitor-company-dropdown',\n",
        "                    options=[{'label': 'All Companies', 'value': 'All'}] + [{'label': c, 'value': c} for c in companies],\n",
        "                    value='All',\n",
        "                    clearable=False,\n",
        "                    className=\"mt-4\"\n",
        "                ), md=4)\n",
        "            ], align=\"center\"),\n",
        "\n",
        "            html.Div(id='visitor-company-dashboard-content'), # Main content updated by callback\n",
        "\n",
        "            # --- Cluster Overview moved to bottom of this tab for consistency ---\n",
        "            # Now fully removed the compact cluster overview here as requested, only the button remains global.\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.Hr(className=\"my-4\")),\n",
        "                dbc.Col(html.H3(\"Understand Visitor Segments and Their Personas\", className=\"mb-3\")),\n",
        "                # The button to view detailed personas is now global (id=\"open-cluster-glossary-modal-visitor-tab\")\n",
        "                # This dbc.Card containing concise overview is now moved inside the visitor_dashboard_content callback IF needed.\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Clustering Diagnostics: Elbow Method (Distortion)\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='elbow-method-chart'))\n",
        "                ]), md=6, className=\"mb-4\"),\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Clustering Diagnostics: Silhouette Scores\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='silhouette-score-chart'))\n",
        "                ]), md=6, className=\"mb-4\"),\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([ # This card now represents the concise overview\n",
        "                    dbc.CardHeader(\"Visitor Cluster Overview (Concise)\"),\n",
        "                    dbc.CardBody([\n",
        "                        html.P(\"Each cluster represents a distinct group of visitors based on their browse and download behaviors. Hover over each for a brief description.\"),\n",
        "                        dbc.Row(\n",
        "                            [dbc.Col(dbc.Card(\n",
        "                                [\n",
        "                                    dbc.CardHeader(f\"Cluster {p['num']}: {p['name']}\"),\n",
        "                                    dbc.CardBody(html.P(p['brief'], className=\"card-text\"), id=f\"cluster-tooltip-target-{p['id']}\"),\n",
        "                                    dbc.Tooltip(dcc.Markdown(p['full']), target=f\"cluster-tooltip-target-{p['id']}\", placement=\"top\")\n",
        "                                ], className=\"mb-2\"\n",
        "                            ), md=int(12/len(cluster_personas_data))) for p in cluster_personas_data]\n",
        "                        ) if cluster_personas_data else html.P(\"No cluster data available.\")\n",
        "                    ])\n",
        "                ], className=\"mb-4\")),\n",
        "            ], className=\"mt-5 mb-4\"),\n",
        "        ]),\n",
        "\n",
        "        # --- Individual Visitor Dashboard ---\n",
        "        dbc.Tab(label=\"Individual Visitor Analysis\", tab_id=\"tab-visitor\", children=[\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.H2(\"Deep Dive into Individual Visitor Behavior\", className=\"my-4\")),\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Visitor Filtering & Search\", className=\"bg-primary text-white\"),\n",
        "                    dbc.CardBody([\n",
        "                        html.P(\"Use the table below to filter, sort, or search for a specific visitor.\", className=\"text-muted\"),\n",
        "                        dash_table.DataTable(\n",
        "                            id='visitor-table',\n",
        "                            columns=[\n",
        "                                {\"name\": \"Visitor ID\", \"id\": \"visitor_id\", \"deletable\": False, \"selectable\": True},\n",
        "                                {\"name\": \"Company\", \"id\": \"company\", \"deletable\": False, \"selectable\": True, \"type\": \"text\"},\n",
        "                                {\"name\": \"Avg Session Depth\", \"id\": \"avg_session_depth\", \"deletable\": False, \"selectable\": True, \"type\": \"numeric\"},\n",
        "                                {\"name\": \"Download Count\", \"id\": \"download_count\", \"deletable\": False, \"selectable\": True, \"type\": \"numeric\"},\n",
        "                                {\"name\": \"Visit Count\", \"id\": \"visit_count\", \"deletable\": False, \"selectable\": True, \"type\": \"numeric\"},\n",
        "                                {\"name\": \"IR Interest Score\", \"id\": \"investor_interest_score\", \"deletable\": False, \"selectable\": True, \"type\": \"numeric\"},\n",
        "                                {\"name\": \"User Tags\", \"id\": \"user_tags\", \"deletable\": False, \"selectable\": True, \"type\": \"text\"}\n",
        "                            ],\n",
        "                            data=df_profiles.to_dict('records') if not df_profiles.empty else [],\n",
        "                            style_table={'overflowX': 'auto'},\n",
        "                            page_action=\"native\",\n",
        "                            page_current=0,\n",
        "                            page_size=15,\n",
        "                            sort_action=\"native\",\n",
        "                            sort_mode=\"multi\",\n",
        "                            filter_action=\"native\",\n",
        "                            row_selectable=\"single\",\n",
        "                            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},\n",
        "                            style_cell={'textAlign': 'left', 'minWidth': '100px', 'width': '100px', 'maxWidth': '180px'},\n",
        "                        )\n",
        "                    ])\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "            html.Div(id='individual-visitor-content')\n",
        "        ]),\n",
        "\n",
        "        # --- Event Impact Analysis Dashboard ---\n",
        "        dbc.Tab(label=\"Event Impact Analysis\", tab_id=\"tab-event-impact\", children=[\n",
        "            dbc.Row([\n",
        "                dbc.Col(html.H2(\"Company A Event Impact on Investor Behavior\", className=\"my-4\")),\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"AI Generated Event Impact Summary\"),\n",
        "                    dbc.CardBody(dcc.Markdown(id='event-impact-ai-summary', className=\"ai-summary-text\"))\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Company A Daily Total Visits Over Time with Financial Events\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='company_a-visits-time-series'))\n",
        "                ]), md=6, className=\"mb-4\"),\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Company A Daily Total Downloads Over Time with Financial Events\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='company_a-downloads-time-series'))\n",
        "                ]), md=6, className=\"mb-4\")\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Company A Event Impact Summary Table\"),\n",
        "                    dbc.CardBody(dash_table.DataTable(\n",
        "                        id='company_a-event-summary-table',\n",
        "                        columns=[{\"name\": i, \"id\": i} for i in df_event_impact_summary.columns] if not df_event_impact_summary.empty else [],\n",
        "                        data=df_event_impact_summary.to_dict('records') if not df_event_impact_summary.empty else [],\n",
        "                        style_table={'overflowX': 'auto'},\n",
        "                        page_action=\"native\",\n",
        "                        page_current=0,\n",
        "                        page_size=10,\n",
        "                        sort_action=\"native\",\n",
        "                        sort_mode=\"multi\",\n",
        "                        style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},\n",
        "                        style_cell={'textAlign': 'left', 'minWidth': '100px', 'width': '100px', 'maxWidth': '180px'},\n",
        "                    ))\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Investor Cluster Response to Financial Events (Change in Visits %)\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='cluster-event-impact-heatmap'))\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Detailed Cluster Event Impact Data\"),\n",
        "                    dbc.CardBody(id='cluster-event-impact-table')\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "            dbc.Row([\n",
        "                dbc.Col(dbc.Card([\n",
        "                    dbc.CardHeader(\"Top Content Sections by Investor Cluster Interest\"),\n",
        "                    dbc.CardBody(dcc.Graph(id='cluster-content-interest-chart'))\n",
        "                ]), width=12, className=\"mb-4\")\n",
        "            ]),\n",
        "        ]) # Closes the last dbc.Tab\n",
        "    ]), # Closes the children list of dbc.Tabs AND the dbc.Tabs component itself.\n",
        "\n",
        "    ## --- NEW FEATURE: Glossary Modals (Global, outside tabs) ---\n",
        "    # Tag Glossary Modal\n",
        "    dbc.Modal(\n",
        "        [\n",
        "            dbc.ModalHeader(\"Glossary of Visitor Tags\"),\n",
        "            dbc.ModalBody(id=\"tag-glossary-modal-body\"),\n",
        "            dbc.ModalFooter(\n",
        "                dbc.Button(\"Close\", id=\"close-tag-glossary-modal\", className=\"ms-auto\")\n",
        "            ),\n",
        "        ],\n",
        "        id=\"tag-glossary-modal\",\n",
        "        size=\"lg\",\n",
        "        centered=True,\n",
        "        scrollable=True,\n",
        "    ),\n",
        "\n",
        "    # Cluster Glossary Modal\n",
        "    dbc.Modal(\n",
        "        [\n",
        "            dbc.ModalHeader(\"Detailed Cluster Personas\"),\n",
        "            dbc.ModalBody(id=\"cluster-glossary-modal-body\"),\n",
        "            dbc.ModalFooter(\n",
        "                dbc.Button(\"Close\", id=\"close-cluster-glossary-modal\", className=\"ms-auto\")\n",
        "            ),\n",
        "        ],\n",
        "        id=\"cluster-glossary-modal\",\n",
        "        size=\"lg\",\n",
        "        centered=True,\n",
        "        scrollable=True,\n",
        "    ),\n",
        "\n",
        "], fluid=True)\n",
        "\n",
        "## --- NEW HELPER FUNCTION: Executive Summary for Website Insights ---\n",
        "def generate_website_executive_summary_content(selected_website, metrics_filtered, global_benchmark_metrics):\n",
        "    summary_lines = []\n",
        "\n",
        "    # Current Performance snapshot\n",
        "    total_visits = metrics_filtered.get('total_visits', 0)\n",
        "    avg_depth = metrics_filtered.get('avg_depth', 0.0)\n",
        "    bounce_rate = metrics_filtered.get('bounce_rate', 0.0)\n",
        "    total_downloads = metrics_filtered.get('total_downloads', 0)\n",
        "\n",
        "    summary_lines.append(f\"**Key Performance for {selected_website}:**\")\n",
        "    summary_lines.append(f\"- **{total_visits}** total visits; users view **{avg_depth:.2f} pages** per session.\")\n",
        "    summary_lines.append(f\"- **{bounce_rate:.2%}** bounce rate indicates a need to engage first-page visitors.\")\n",
        "    summary_lines.append(f\"- Recorded **{total_downloads} downloads**, showing explicit content interest.\")\n",
        "\n",
        "    # Comparisons with Overall/Benchmark\n",
        "    bench_avg_depth = global_benchmark_metrics.get('avg_depth', 0.0)\n",
        "    bench_bounce_rate = global_benchmark_metrics.get('bounce_rate', 0.0)\n",
        "    bench_downloads_per_visit = global_benchmark_metrics.get('avg_downloads_per_visit', 0.0)\n",
        "    current_downloads_per_visit = total_downloads / total_visits if total_visits > 0 else 0\n",
        "\n",
        "    summary_lines.append(\"\\n**Compared to Overall Averages:**\")\n",
        "    if avg_depth > bench_avg_depth:\n",
        "        summary_lines.append(f\"- **Higher Session Depth ({avg_depth:.2f} vs Global Avg: {bench_avg_depth:.2f} pages)**: This website keeps users more engaged per session.\")\n",
        "    else:\n",
        "        summary_lines.append(f\"- **Lower Session Depth ({avg_depth:.2f} vs Global Avg: {bench_avg_depth:.2f} pages)**: Consider internal linking and richer content to encourage deeper exploration.\")\n",
        "\n",
        "    if bounce_rate < bench_bounce_rate:\n",
        "        summary_lines.append(f\"- **Lower Bounce Rate ({bounce_rate:.2%} vs Global Avg: {bench_bounce_rate:.2%})**: This is excellent! It suggests landing pages are highly effective, retaining visitors and encouraging them to proceed beyond the entry page.\")\n",
        "    else:\n",
        "        summary_lines.append(f\"- **Higher Bounce Rate ({bounce_rate:.2%} vs Global Avg: {bench_bounce_rate:.2%})**: Optimize initial page content and load times for new visitors.\")\n",
        "\n",
        "    if current_downloads_per_visit > bench_downloads_per_visit:\n",
        "        summary_lines.append(f\"- **Higher Download-per-Visit Ratio ({current_downloads_per_visit:.2f} vs Global Avg: {bench_downloads_per_visit:.2f} downloads/visit)**: High-value content is effectively driving conversions.\")\n",
        "    else:\n",
        "        summary_lines.append(f\"- **Lower Download-per-Visit Ratio ({current_downloads_per_visit:.2f} vs Global Avg: {bench_downloads_per_visit:.2f} downloads/visit)**: Improve visibility or appeal of downloadable resources.\")\n",
        "\n",
        "    # Top Content and Audience Insight\n",
        "    top_pageview_section = metrics_filtered['pageview'].index[0] if not metrics_filtered['pageview'].empty else 'N/A'\n",
        "    if top_pageview_section != 'N/A':\n",
        "        summary_lines.append(f\"\\n**Top Content & Audience:**\")\n",
        "        summary_lines.append(f\"- **'{top_pageview_section}'** is the most visited section, a key area for engagement.\")\n",
        "    \n",
        "    if not metrics_filtered['tag_counts'].empty:\n",
        "        top_tag = metrics_filtered['tag_counts'].index[0]\n",
        "        if top_tag in tag_explanations:\n",
        "            summary_lines.append(f\"- A significant portion of users are **'{top_tag}'** (meaning: {tag_explanations[top_tag].split(',')[0].strip()}). Tailor content or CTAs for this audience.\")\n",
        "\n",
        "\n",
        "    summary_lines.append(\"\\n**Actionable Insight:** Focus on optimizing the **user journey from landing pages to deeper content** to reduce bounce rate and increase session depth. For **downloadable assets**, ensure their visibility and relevance are maximized.\")\n",
        "\n",
        "    return dbc.Card([\n",
        "        dbc.CardHeader(\"Executive Summary: Quick Insights\", className=\"bg-info text-white\"),\n",
        "        dbc.CardBody(dcc.Markdown(\"\\n\".join(summary_lines), className=\"ai-summary-text\")),\n",
        "    ], className=\"mb-4\")\n",
        "\n",
        "# --- Callback for Website Company Insights Dashboard ---\n",
        "@app.callback(\n",
        "    # Outputting to three distinct components:\n",
        "    Output('website-company-dashboard-content', 'children'),\n",
        "    Output('website-executive-summary-container', 'children'), \n",
        "    Output('tag-behavior-table-container', 'children'), \n",
        "    Input('website-company-dropdown', 'value')\n",
        ")\n",
        "def update_website_company_dashboard(selected_website):\n",
        "    df_filtered_website = df_all_with_profiles if selected_website == 'All' else df_all_with_profiles[df_all_with_profiles['website_company'] == selected_website]\n",
        "    \n",
        "    if df_filtered_website.empty:\n",
        "        return (\n",
        "            dbc.Alert(f\"No data available for website: {selected_website}\", color=\"warning\"), \n",
        "            html.Div(), # Return an empty div for Executive Summary\n",
        "            html.P(\"No data for tag behavior table.\") # Return text for tag table\n",
        "        )\n",
        "\n",
        "    analyzer_filtered_website = BehaviorAnalyzer(df_filtered_website)\n",
        "\n",
        "    cluster_content_interest_df = analyzer_filtered_website.analyze_content_interest_by_cluster()\n",
        "    \n",
        "    # Map Cluster ID to Cluster Name for readability in AI summary and potential future display\n",
        "    global cluster_personas_data # Access the global cluster personas data\n",
        "    if not cluster_content_interest_df.empty and cluster_personas_data:\n",
        "        cluster_id_to_name_map = {p['num']: p['name'] for p in cluster_personas_data}\n",
        "        cluster_content_interest_df['Cluster Name'] = cluster_content_interest_df['Cluster ID'].map(cluster_id_to_name_map).fillna('Unclustered')\n",
        "    else:\n",
        "        # If no cluster data or personas, ensure it's an empty DataFrame with expected columns if needed downstream\n",
        "        cluster_content_interest_df = pd.DataFrame(columns=['Cluster ID', 'Section', 'Interest Score', 'Cluster Name'])\n",
        "\n",
        "    website_benchmark_metrics = {}\n",
        "    for wc in website_companies:\n",
        "        df_temp = df_all_with_profiles[df_all_with_profiles['website_company'] == wc]\n",
        "        if df_temp.empty:\n",
        "            continue\n",
        "        temp_analyzer = BehaviorAnalyzer(df_temp)\n",
        "        visits = df_temp['id_visit'].nunique()\n",
        "        downloads = df_temp['download_flag'].sum()\n",
        "        temp_bounce_rates = temp_analyzer.calculate_bounce_rate()['metrics_data']['overall_bounce_rate']\n",
        "        temp_avg_depth = temp_analyzer.calculate_average_depth()['metrics_data']['avg_depth_per_visit']\n",
        "\n",
        "        website_benchmark_metrics[wc] = {\n",
        "            'avg_depth': temp_avg_depth,\n",
        "            'bounce_rate': temp_bounce_rates,\n",
        "            'total_downloads': downloads,\n",
        "            'visits': visits\n",
        "        }\n",
        "\n",
        "    global_analyzer = BehaviorAnalyzer(df_processed)  \n",
        "    global_avg_depth_results = global_analyzer.calculate_average_depth()['metrics_data']\n",
        "    global_bounce_results = global_analyzer.calculate_bounce_rate()['metrics_data']\n",
        "\n",
        "    global_avg_downloads_per_visit_for_ai_text = df_processed['download_flag'].sum() / df_processed['id_visit'].nunique() if df_processed['id_visit'].nunique() > 0 else 0.0\n",
        "\n",
        "    all_website_total_downloads_list = [m['total_downloads'] for m in website_benchmark_metrics.values()]\n",
        "    global_bench_downloads_for_kpi = np.mean(all_website_total_downloads_list) if all_website_total_downloads_list else 0.0\n",
        "\n",
        "    global_bench_avg_depth = global_avg_depth_results['avg_depth_per_visit']\n",
        "    global_bench_bounce_rate = global_bounce_results['overall_bounce_rate']\n",
        "\n",
        "    filtered_visitor_ids = df_filtered_website['visitor_id'].unique()\n",
        "    df_profiles_filtered_website = df_profiles[df_profiles['visitor_id'].isin(filtered_visitor_ids)]\n",
        "    \n",
        "    metrics_filtered_website = {\n",
        "        'pageview': analyzer_filtered_website.df['section'].value_counts(),\n",
        "        'time': analyzer_filtered_website.df['timestamp'].dt.hour.value_counts().sort_index(),\n",
        "        'bounce_rate': analyzer_filtered_website.calculate_bounce_rate()['metrics_data']['overall_bounce_rate'],\n",
        "        'avg_depth': analyzer_filtered_website.calculate_average_depth()['metrics_data']['avg_depth_per_visit'],\n",
        "        'total_visits': df_filtered_website['id_visit'].nunique(),\n",
        "        'unique_visitors': df_filtered_website['visitor_id'].nunique(),\n",
        "        'total_downloads': df_filtered_website['download_flag'].sum(),\n",
        "        'new_vs_returning_comp': analyzer_filtered_website.compare_new_vs_returning()['metrics_data'],\n",
        "        'tag_counts': df_profiles_filtered_website['user_tags'].str.split(',').explode().str.strip().value_counts() if not df_profiles_filtered_website.empty else pd.Series()\n",
        "    }\n",
        "    \n",
        "    company_pageviews_pct = (metrics_filtered_website['pageview'] / metrics_filtered_website['pageview'].sum() * 100).reset_index(name='percentage') if not metrics_filtered_website['pageview'].empty and metrics_filtered_website['pageview'].sum() > 0 else pd.DataFrame(columns=['section', 'percentage'])\n",
        "    company_pageviews_pct.rename(columns={'index': 'section'}, inplace=True)\n",
        "    company_pageviews_pct['type'] = selected_website\n",
        "    \n",
        "    overall_pageviews_pct = (df_processed['section'].value_counts() / len(df_processed) * 100).reset_index(name='percentage') if not df_processed['section'].empty and len(df_processed) > 0 else pd.DataFrame(columns=['section', 'percentage'])\n",
        "    overall_pageviews_pct.rename(columns={'index': 'section'}, inplace=True)\n",
        "    overall_pageviews_pct['type'] = 'Overall'\n",
        "    \n",
        "    combined_pageviews_for_ai = pd.concat([company_pageviews_pct, overall_pageviews_pct], ignore_index=True)\n",
        "\n",
        "    ai_analysis_text = generate_website_ai_analysis(\n",
        "        selected_website,\n",
        "        metrics_filtered_website,\n",
        "        {'avg_depth': global_bench_avg_depth,\n",
        "         'bounce_rate': global_bench_bounce_rate,\n",
        "         'avg_downloads_per_visit': global_avg_downloads_per_visit_for_ai_text \n",
        "        },\n",
        "        tag_explanations,\n",
        "        combined_pageviews_for_ai,\n",
        "        cluster_content_interest_df=cluster_content_interest_df\n",
        "    )\n",
        "\n",
        "    tag_behavior_data = []\n",
        "    for tag in all_tags:\n",
        "        visitors_with_tag_profiles = df_profiles_filtered_website[df_profiles_filtered_website['user_tags'].str.contains(tag, na=False)]\n",
        "        if not visitors_with_tag_profiles.empty:\n",
        "            visitors_with_tag_ids = visitors_with_tag_profiles['visitor_id'].unique()\n",
        "            df_tag_filtered = df_filtered_website[df_filtered_website['visitor_id'].isin(visitors_with_tag_ids)]\n",
        "            \n",
        "            if not df_tag_filtered.empty:\n",
        "                tag_analyzer = BehaviorAnalyzer(df_tag_filtered)\n",
        "                \n",
        "                tag_avg_depth = tag_analyzer.calculate_average_depth()['metrics_data']['avg_depth_per_visit']\n",
        "                tag_bounce_rate = tag_analyzer.calculate_bounce_rate()['metrics_data']['overall_bounce_rate']\n",
        "                tag_total_downloads = tag_analyzer.df['download_flag'].sum()  \n",
        "                tag_unique_visitors = visitors_with_tag_profiles['visitor_id'].nunique()    \n",
        "\n",
        "                tag_behavior_data.append({\n",
        "                    'Tag': tag,\n",
        "                    'Unique Visitors': tag_unique_visitors,\n",
        "                    'Avg Session Depth': round(tag_avg_depth, 2),\n",
        "                    'Bounce Rate': f\"{tag_bounce_rate:.2%}\",\n",
        "                    'Total Downloads': int(tag_total_downloads)\n",
        "                })\n",
        "\n",
        "    df_tag_behavior = pd.DataFrame(tag_behavior_data)\n",
        "    if not df_tag_behavior.empty:\n",
        "        df_tag_behavior = df_tag_behavior.sort_values(by='Avg Session Depth', ascending=False)\n",
        "\n",
        "\n",
        "    tag_comparison_table_content = dash_table.DataTable(\n",
        "        id='tag-behavior-table', # Keep ID for potential future interactivity\n",
        "        columns=[{\"name\": col, \"id\": col} for col in df_tag_behavior.columns],\n",
        "        data=df_tag_behavior.to_dict('records'),\n",
        "        style_table={'overflowX': 'auto'},\n",
        "        page_action=\"native\",\n",
        "        page_current=0,\n",
        "        page_size=10,  \n",
        "        sort_action=\"native\",\n",
        "        sort_mode=\"multi\",  \n",
        "        style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},\n",
        "        style_cell={'textAlign': 'left', 'minWidth': '120px', 'width': '120px', 'maxWidth': '180px'},\n",
        "    ) if not df_tag_behavior.empty else html.P(\"No sufficient tag behavior data available for analysis for the selected website.\")\n",
        "\n",
        "\n",
        "    # --- Dashboard Layout Structure for Website Company Insights ---\n",
        "    # This is the content returned for 'website-company-dashboard-content'\n",
        "    # It contains all the KPIs and plots.\n",
        "    website_dashboard_content = html.Div([\n",
        "        # 1. KPI Card Layout with Benchmarks\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Total Visits\"),\n",
        "                dbc.CardBody(html.H4(f\"{metrics_filtered_website['total_visits']}\", className=\"text-center text-secondary\")),\n",
        "                dbc.CardFooter(f\"Total sessions for {selected_website}\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Avg Session Depth\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{metrics_filtered_website['avg_depth']:.2f} Pages\", className=\"text-center text-info\"),\n",
        "                    html.P(f\"(Benchmark Avg: {global_bench_avg_depth:.2f})\", className=\"text-center text-muted\") if selected_website != 'All' else None\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Avg pages visited per session\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Bounce Rate\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{metrics_filtered_website['bounce_rate']:.2%}\", className=\"text-center text-danger\"),\n",
        "                    html.P(f\"(Benchmark Avg: {global_bench_bounce_rate:.2%})\", className=\"text-center text-muted\") if selected_website != 'All' else None\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Single-page visit percentage\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Downloads\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{int(metrics_filtered_website['total_downloads'])}\", className=\"text-center text-success\"),\n",
        "                    html.P(f\"(Benchmark Avg: {global_bench_downloads_for_kpi:.1f})\", className=\"text-center text-muted\") if selected_website != 'All' else None\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Total file downloads\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "        ], className=\"mb-4\"),\n",
        "        # NEW: AI Analysis Card\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"AI Website Insight Analysis\", className=\"bg-primary text-white\"),\n",
        "                dbc.CardBody(dcc.Markdown(ai_analysis_text, className=\"ai-analysis-text\")),\n",
        "            ]), width=12, className=\"mb-4\")\n",
        "        ]),\n",
        "        # Row 1: Pageviews Comparison & Time Distribution\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Pageview Percentage Comparison (Website vs. Overall)\"),\n",
        "                dbc.CardBody(dcc.Graph(figure=px.bar(\n",
        "                    combined_pageviews_for_ai, \n",
        "                    x='section', \n",
        "                    y='percentage',\n",
        "                    color='type',\n",
        "                    barmode='group',\n",
        "                    title=\"Pageview Distribution by Section (%)\",\n",
        "                    labels={'percentage': 'Pageviews (%)', 'section': 'Website Section', 'type': 'Data Set'},\n",
        "                ) if not combined_pageviews_for_ai.empty else go.Figure().update_layout(title=\"Not enough data for this plot\"))),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Visits by Hour of Day\"),\n",
        "                dbc.CardBody(dcc.Graph(figure=px.line(\n",
        "                    x=metrics_filtered_website['time'].index,\n",
        "                    y=metrics_filtered_website['time'].values,\n",
        "                    title=\"Visitor Trend by Hour\",\n",
        "                    labels={'x': 'Hour of Day', 'y': 'Visits'},\n",
        "                ) if not metrics_filtered_website['time'].empty else go.Figure().update_layout(title=\"Not enough data for this plot\"))),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "        # Row 2: Visitor Tags & New vs. Returning\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Visitor Behavior Tag Distribution\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    id='tag-distribution-graph-website', \n",
        "                    figure=px.pie(\n",
        "                        names=metrics_filtered_website['tag_counts'].index,\n",
        "                        values=metrics_filtered_website['tag_counts'].values,\n",
        "                        title=\"Visitor Tag Proportions\",\n",
        "                    ) if not metrics_filtered_website['tag_counts'].empty else go.Figure().update_layout(title=\"No visitor tags to display\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"New vs. Returning Visitor Comparison\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.bar(\n",
        "                        x=['New Visitors', 'Returning Visitors'],\n",
        "                        y=[metrics_filtered_website['new_vs_returning_comp']['new_depth_comparison'], metrics_filtered_website['new_vs_returning_comp']['returning_depth_comparison']],\n",
        "                        title=\"Average Session Depth Comparison\",\n",
        "                        labels={'x': 'Visitor Type', 'y': 'Average Depth'},\n",
        "                    )\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "        # Row 3: Sitemap Heatmap & Hierarchical Chart\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Website Navigation Heatmap\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.sunburst(\n",
        "                        df_filtered_website,\n",
        "                        path=['section', 'sub-section'],\n",
        "                        values='id_visit',\n",
        "                        title='Sitemap Clicks Heatmap'\n",
        "                    ) if not df_filtered_website.empty else go.Figure().update_layout(title=\"Not enough data for this plot\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Investor Profile Engagement by Content Area\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.sunburst(\n",
        "                        df_filtered_website,\n",
        "                        path=['cluster', 'section'],\n",
        "                        values='id_visit',\n",
        "                        title='Cluster-Based Content Engagement'\n",
        "                    ) if not df_filtered_website.empty and 'cluster' in df_filtered_website.columns and not df_filtered_website['cluster'].nunique() == 1 else go.Figure().update_layout(title=\"Not enough data for this plot\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "    ])\n",
        "    \n",
        "    return (\n",
        "        website_dashboard_content,\n",
        "        generate_website_executive_summary_content(selected_website, metrics_filtered_website, {'avg_depth': global_bench_avg_depth, 'bounce_rate': global_bench_bounce_rate, 'avg_downloads_per_visit': global_avg_downloads_per_visit_for_ai_text}),\n",
        "        tag_comparison_table_content\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Callback for Visitor Company Insights Dashboard ---\n",
        "@app.callback(\n",
        "    # Only one output for the entire dashboard content\n",
        "    Output('visitor-company-dashboard-content', 'children'),\n",
        "    Input('visitor-company-dropdown', 'value')\n",
        ")\n",
        "def update_visitor_company_dashboard(selected_company):\n",
        "    df_filtered = df_all_with_profiles if selected_company == 'All' else df_all_with_profiles[df_all_with_profiles['company'] == selected_company]\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        return dbc.Alert(f\"No data available for company: {selected_company}\", color=\"warning\")\n",
        "\n",
        "    filtered_visitor_ids = df_filtered['visitor_id'].unique()\n",
        "    df_profiles_filtered = df_profiles[df_profiles['visitor_id'].isin(filtered_visitor_ids)]\n",
        "    \n",
        "    analyzer_filtered = BehaviorAnalyzer(df_filtered)\n",
        "\n",
        "    metrics_filtered = {\n",
        "        'pageview': analyzer_filtered.df['section'].value_counts(),\n",
        "        'time': analyzer_filtered.df['timestamp'].dt.hour.value_counts().sort_index(),\n",
        "        'bounce_rate': analyzer_filtered.calculate_bounce_rate()['metrics_data']['overall_bounce_rate'],\n",
        "        'avg_depth': analyzer_filtered.calculate_average_depth()['metrics_data']['avg_depth_per_visit'],\n",
        "        'total_visits': df_filtered['id_visit'].nunique(),\n",
        "        'unique_visitors': df_filtered['visitor_id'].nunique(),\n",
        "        'total_downloads': df_filtered['download_flag'].sum(),\n",
        "        'new_vs_returning_comp': analyzer_filtered.compare_new_vs_returning()['metrics_data'],\n",
        "        'tag_counts': df_profiles_filtered['user_tags'].str.split(',').explode().str.strip().value_counts() if not df_profiles_filtered.empty else pd.Series()\n",
        "    }\n",
        "    \n",
        "    company_pageviews_pct = (metrics_filtered['pageview'] / metrics_filtered['pageview'].sum() * 100).reset_index(name='percentage') if not metrics_filtered['pageview'].empty and metrics_filtered['pageview'].sum() > 0 else pd.DataFrame(columns=['section', 'percentage'])\n",
        "    company_pageviews_pct.rename(columns={'index': 'section'}, inplace=True)\n",
        "    company_pageviews_pct['type'] = selected_company\n",
        "    \n",
        "    overall_pageviews_pct = (df_processed['section'].value_counts() / len(df_processed) * 100).reset_index(name='percentage') if not df_processed['section'].empty and len(df_processed) > 0 else pd.DataFrame(columns=['section', 'percentage'])\n",
        "    overall_pageviews_pct.rename(columns={'index': 'section'}, inplace=True)\n",
        "    overall_pageviews_pct['type'] = 'Overall'\n",
        "    \n",
        "    combined_pageviews = pd.concat([company_pageviews_pct, overall_pageviews_pct], ignore_index=True)\n",
        "    \n",
        "    # This is the complete content returned for 'visitor-company-dashboard-content'\n",
        "    visitor_dashboard_content = html.Div([\n",
        "        # 1. KPI Card Layout with Benchmarks\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Total Visits\"),\n",
        "                dbc.CardBody(html.H4(f\"{metrics_filtered['total_visits']}\", className=\"text-center text-secondary\")),\n",
        "                dbc.CardFooter(\"Total sessions for this company\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Avg Session Depth\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{metrics_filtered['avg_depth']:.2f} Pages\", className=\"text-center text-info\"),\n",
        "                    html.P(f\"(Overall Avg: {overall_metrics['avg_depth_results']['avg_depth_per_visit']:.2f})\", className=\"text-center text-muted\")\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Avg pages visited per session\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Bounce Rate\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{metrics_filtered['bounce_rate']:.2%}\", className=\"text-center text-danger\"),\n",
        "                    html.P(f\"(Overall Avg: {overall_metrics['bounce_results']['overall_bounce_rate']:.2f})\", className=\"text-center text-muted\")\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Single-page visit percentage\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Downloads\"),\n",
        "                dbc.CardBody([\n",
        "                    html.H4(f\"{int(metrics_filtered['total_downloads'])}\", className=\"text-center text-success\"),\n",
        "                    html.P(f\"(Overall Avg: {overall_visitor_averages.get('download_count', 0):.1f})\", className=\"text-center text-muted\")\n",
        "                ]),\n",
        "                dbc.CardFooter(\"Total file downloads\", className=\"text-muted\")\n",
        "            ], className=\"h-100\"), md=3),\n",
        "        ], className=\"mb-4\"),\n",
        "\n",
        "        # 2. Interactive Charts with Benchmarks\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Pageview Percentage Comparison (Company vs. Overall)\"),\n",
        "                dbc.CardBody(dcc.Graph(figure=px.bar(\n",
        "                    combined_pageviews, \n",
        "                    x='section', \n",
        "                    y='percentage',\n",
        "                    color='type',\n",
        "                    barmode='group',\n",
        "                    title=\"Pageview Distribution by Section (%)\",\n",
        "                    labels={'percentage': 'Pageviews (%)', 'section': 'Website Section', 'type': 'Data Set'},\n",
        "                ) if not combined_pageviews.empty else go.Figure().update_layout(title=\"Not enough data for this plot\"))),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Visits by Hour of Day\"),\n",
        "                dbc.CardBody(dcc.Graph(figure=px.line(\n",
        "                    x=metrics_filtered['time'].index,\n",
        "                    y=metrics_filtered['time'].values,\n",
        "                    title=\"Visitor Trend by Hour\",\n",
        "                    labels={'x': 'Hour of Day', 'y': 'Visits'},\n",
        "                ) if not metrics_filtered['time'].empty else go.Figure().update_layout(title=\"Not enough data for this plot\"))),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Visitor Behavior Tag Distribution\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    id='tag-distribution-graph-visitor', \n",
        "                    figure=px.pie(\n",
        "                        names=metrics_filtered['tag_counts'].index,\n",
        "                        values=metrics_filtered['tag_counts'].values,\n",
        "                        title=\"Visitor Tag Proportions\",\n",
        "                    ) if not metrics_filtered['tag_counts'].empty else go.Figure().update_layout(title=\"No visitor tags to display\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"New vs. Returning Visitor Comparison\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.bar(\n",
        "                        x=['New Visitors', 'Returning Visitors'],\n",
        "                        y=[metrics_filtered['new_vs_returning_comp']['new_depth_comparison'], metrics_filtered['new_vs_returning_comp']['returning_depth_comparison']],\n",
        "                        title=\"Average Session Depth Comparison\",\n",
        "                        labels={'x': 'Visitor Type', 'y': 'Average Depth'},\n",
        "                    )\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "        dbc.Row([\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Website Navigation Heatmap\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.sunburst(\n",
        "                        df_filtered,\n",
        "                        path=['section', 'sub-section'],\n",
        "                        values='id_visit',\n",
        "                        title='Sitemap Clicks Heatmap'\n",
        "                    ) if not df_filtered.empty else go.Figure().update_layout(title=\"Not enough data for this plot\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "            dbc.Col(dbc.Card([\n",
        "                dbc.CardHeader(\"Investor Profile Engagement by Content Area\"),\n",
        "                dbc.CardBody(dcc.Graph(\n",
        "                    figure=px.sunburst(\n",
        "                        df_filtered,\n",
        "                        path=['cluster', 'section'],\n",
        "                        values='id_visit',\n",
        "                        title='Cluster-Based Content Engagement'\n",
        "                    ) if not df_filtered.empty and 'cluster' in df_filtered.columns and not df_filtered['cluster'].nunique() == 1 else go.Figure().update_layout(title=\"Not enough data for this plot\")\n",
        "                )),\n",
        "            ]), md=6, className=\"mb-4\"),\n",
        "        ]),\n",
        "    ])\n",
        "    return visitor_dashboard_content # Return the complete content for this tab\n",
        "\n",
        "@app.callback(\n",
        "    Output('elbow-method-chart', 'figure'),\n",
        "    Output('silhouette-score-chart', 'figure'),\n",
        "    Input('tabs-main', 'active_tab')\n",
        ")\n",
        "def update_clustering_diagnostics(active_tab):\n",
        "    global overall_metrics\n",
        "\n",
        "    # Check if we have the necessary data\n",
        "    if 'clustering_results' not in overall_metrics or 'clustering_scores' not in overall_metrics['clustering_results']:\n",
        "        return go.Figure().update_layout(title=\"Clustering data not available\"), go.Figure().update_layout(title=\"Clustering data not available\")\n",
        "\n",
        "    score_data = pd.DataFrame(overall_metrics['clustering_results']['clustering_scores'])\n",
        "\n",
        "    # Elbow Method Chart\n",
        "    elbow_fig = px.line(score_data, x='n_clusters', y='Distortion (Inertia)', markers=True,\n",
        "                         title='Elbow Method for Optimal K',\n",
        "                         labels={'n_clusters': 'Number of Clusters (k)', 'Distortion (Inertia)': 'Inertia'},\n",
        "                         template='plotly_white')\n",
        "    elbow_fig.update_layout(xaxis_title=\"Number of Clusters (k)\")\n",
        "    elbow_fig.update_layout(yaxis_title=\"Inertia (Distortion)\")\n",
        "\n",
        "    # Silhouette Score Chart\n",
        "    silhouette_fig = px.line(score_data.dropna(), x='n_clusters', y='Silhouette Score', markers=True,\n",
        "                             title='Silhouette Scores for Optimal K',\n",
        "                             labels={'n_clusters': 'Number of Clusters (k)', 'Silhouette Score': 'Silhouette Score'},\n",
        "                             template='plotly_white')\n",
        "    silhouette_fig.update_layout(xaxis_title=\"Number of Clusters (k)\")\n",
        "    silhouette_fig.update_layout(yaxis_title=\"Silhouette Score\")\n",
        "    \n",
        "    # Highlight the best k value (if available)\n",
        "    if not score_data['Silhouette Score'].isnull().all():\n",
        "        best_k = score_data.loc[score_data['Silhouette Score'].idxmax(), 'n_clusters']\n",
        "        silhouette_fig.add_vline(x=best_k, line_width=2, line_dash=\"dash\", line_color=\"red\",\n",
        "                                 annotation_text=f\"Optimal k = {best_k}\", annotation_position=\"top right\")\n",
        "\n",
        "    return elbow_fig, silhouette_fig\n",
        "\n",
        "@app.callback(\n",
        "    Output('individual-visitor-content', 'children'),\n",
        "    Input('visitor-table', 'selected_rows')\n",
        ")\n",
        "def update_individual_visitor_stats(selected_rows):\n",
        "    if not selected_rows:\n",
        "        return html.Div([html.H4(\"No Visitor Selected\"), html.P(\"Please select a row in the table above to view the visitor's details.\")])\n",
        "        \n",
        "    selected_row_index = selected_rows[0]\n",
        "    selected_visitor_id = df_profiles.iloc[selected_row_index]['visitor_id']\n",
        "    \n",
        "    try:\n",
        "        user_profile_df = df_profiles[df_profiles['visitor_id'] == selected_visitor_id].copy()\n",
        "        if user_profile_df.empty:\n",
        "            return html.Div([html.H4(\"Visitor Profile Not Found\"), html.P(f\"No detailed profile found for Visitor ID: {selected_visitor_id}.\")])\n",
        "        user_profile = user_profile_df.iloc[0]\n",
        "\n",
        "        for col in BehaviorAnalyzer.CLUSTERING_FEATURES:\n",
        "            if col not in user_profile:\n",
        "                user_profile[col] = 0.0\n",
        "\n",
        "        analyzer = BehaviorAnalyzer(df_processed) \n",
        "        user_story = analyzer.generate_user_story(selected_visitor_id, overall_metrics_dict=overall_metrics, overall_visitor_averages=overall_visitor_averages)\n",
        "        \n",
        "        visitor_company = user_profile.get('company', 'N/A')\n",
        "\n",
        "        user_visits = df_processed[df_processed['visitor_id'] == selected_visitor_id].sort_values(['id_visit', 'timestamp'])\n",
        "        user_visits_cleaned = user_visits.dropna(subset=['section'])\n",
        "        user_visits_cleaned = user_visits_cleaned[user_visits_cleaned['section'].str.lower() != 'nan'] \n",
        "        \n",
        "        if user_visits_cleaned.empty:\n",
        "            sankey_fig = go.Figure()\n",
        "            sankey_fig.update_layout(title=\"No valid path data\")\n",
        "        else:\n",
        "            paths = user_visits_cleaned.groupby('id_visit')['section'].apply(lambda x: ' -> '.join(x.astype(str))).tolist()\n",
        "            path_counts = pd.Series(paths).value_counts().reset_index()\n",
        "            path_counts.columns = ['path', 'count']\n",
        "            \n",
        "            nodes = pd.unique(path_counts['path'].str.split(' -> ').explode()).tolist()\n",
        "            node_to_id = {node: i for i, node in enumerate(nodes)}\n",
        "            \n",
        "            source = []\n",
        "            target = []\n",
        "            value = []\n",
        "            \n",
        "            for _, row in path_counts.iterrows():\n",
        "                path_segments = row['path'].split(' -> ')\n",
        "                for i in range(len(path_segments) - 1):\n",
        "                    source.append(node_to_id.get(path_segments[i], None))\n",
        "                    target.append(node_to_id.get(path_segments[i+1], None))\n",
        "                    value.append(row['count'])\n",
        "            \n",
        "            valid_indices = [i for i, x in enumerate(source) if x is not None and target[i] is not None]\n",
        "            source = [source[i] for i in valid_indices]\n",
        "            target = [target[i] for i in valid_indices]\n",
        "            value = [value[i] for i in valid_indices]\n",
        "\n",
        "            sankey_fig = go.Figure(go.Sankey(\n",
        "                node=dict(\n",
        "                    pad=15, thickness=20,\n",
        "                    line=dict(color=\"black\", width=0.5),\n",
        "                    label=nodes,\n",
        "                ),\n",
        "                link=dict(\n",
        "                    source=source,\n",
        "                    target=target,\n",
        "                    value=value,\n",
        "                )\n",
        "            ))\n",
        "            sankey_fig.update_layout(title_text=f\"Visitor ID {selected_visitor_id} Path Flow\", font_size=10)\n",
        "\n",
        "        profile_data = {\n",
        "            'Metric': ['Average Session Depth', 'Download Count', 'IR Interest Score', 'Content Breadth', 'Visit Count', 'Repeat Visitor'],\n",
        "            'This Visitor': [\n",
        "                f\"{user_profile.get('avg_session_depth', 0.0):.2f}\", \n",
        "                f\"{int(user_profile.get('download_count', 0.0))}\", \n",
        "                f\"{int(user_profile.get('investor_interest_score', 0.0))}\", \n",
        "                f\"{int(user_profile.get('content_breadth', 0.0))}\",\n",
        "                f\"{int(user_profile.get('visit_count', 0.0))}\",\n",
        "                f\"{'Yes' if user_profile.get('is_repeat_visitor', 0) > 0 else 'No'}\"\n",
        "            ],\n",
        "            'Overall Average': [\n",
        "                f\"{overall_visitor_averages.get('avg_session_depth', 0.0):.2f}\", \n",
        "                f\"{overall_visitor_averages.get('download_count', 0.0):.1f}\", \n",
        "                f\"{overall_visitor_averages.get('investor_interest_score', 0.0):.1f}\", \n",
        "                f\"{overall_visitor_averages.get('content_breadth', 0.0):.1f}\",\n",
        "                f\"{overall_visitor_averages.get('visit_count', 0.0):.1f}\",\n",
        "                f\"{overall_visitor_averages.get('is_repeat_visitor', 0.0):.2%}\" \n",
        "            ],\n",
        "        }\n",
        "\n",
        "        radar_data = pd.DataFrame({\n",
        "            'variable': ['Session Depth', 'Downloads', 'IR Interest', 'Content Breadth'],\n",
        "            'visitor_value': [\n",
        "                user_profile.get('avg_session_depth', 0.0),\n",
        "                user_profile.get('download_count', 0.0),\n",
        "                user_profile.get('investor_interest_score', 0.0),\n",
        "                user_profile.get('content_breadth', 0.0)\n",
        "            ],\n",
        "            'overall_avg': [\n",
        "                overall_visitor_averages.get('avg_session_depth', 0.0),\n",
        "                overall_visitor_averages.get('download_count', 0.0),\n",
        "                overall_visitor_averages.get('investor_interest_score', 0.0),\n",
        "                overall_visitor_averages.get('content_breadth', 0.0)\n",
        "            ]\n",
        "        })\n",
        "        \n",
        "        def min_max_scale(df_series):\n",
        "            if df_series.empty or (df_series.max() - df_series.min()) == 0:\n",
        "                return pd.Series([0] * len(df_series), index=df_series.index)\n",
        "            return (df_series - df_series.min()) / (df_series.max() - df_series.min())\n",
        "        \n",
        "        df_normalized = pd.DataFrame({\n",
        "            'variable': radar_data['variable'],\n",
        "            'visitor_value': min_max_scale(radar_data['visitor_value']),\n",
        "            'overall_avg': min_max_scale(radar_data['overall_avg'])\n",
        "        })\n",
        "        \n",
        "        radar_fig = go.Figure()\n",
        "        radar_fig.add_trace(go.Scatterpolar(\n",
        "            r=df_normalized['visitor_value'],\n",
        "            theta=df_normalized['variable'],\n",
        "            fill='toself',\n",
        "            name='This Visitor'\n",
        "        ))\n",
        "        radar_fig.add_trace(go.Scatterpolar(\n",
        "            r=df_normalized['overall_avg'],\n",
        "            theta=df_normalized['variable'],\n",
        "            fill='toself',\n",
        "            name='Overall Average'\n",
        "        ))\n",
        "        radar_fig.update_layout(\n",
        "            polar=dict(\n",
        "                radialaxis=dict(visible=True, range=[-0.1, 1.1]), \n",
        "                bgcolor='rgba(255, 255, 255, 0)' \n",
        "            ),\n",
        "            showlegend=True,\n",
        "            title='Visitor Behavior vs. Overall Average (Normalized)'\n",
        "        )\n",
        "\n",
        "        user_tags_list = user_profile.get('user_tags', '').split(',')\n",
        "        tag_badges = []\n",
        "        for i, tag in enumerate(user_tags_list):\n",
        "            if not tag:\n",
        "                continue\n",
        "            badge_id = f\"tag-badge-{selected_visitor_id}-{i}\" \n",
        "            tag_badges.append(\n",
        "                dbc.Badge(\n",
        "                    tag,\n",
        "                    color=\"info\",\n",
        "                    className=\"me-1\",\n",
        "                    id=badge_id\n",
        "                )\n",
        "            )\n",
        "\n",
        "            tooltip_content = [html.P(tag_explanations.get(tag.strip(), \"No explanation available.\"))]\n",
        "            \n",
        "            similar_visitors = df_profiles[df_profiles['user_tags'].str.contains(tag, case=False, na=False)]\n",
        "            similar_visitors = similar_visitors[similar_visitors['visitor_id'] != selected_visitor_id] \n",
        "            \n",
        "            if not similar_visitors.empty:\n",
        "                tooltip_content.append(html.Hr())\n",
        "                tooltip_content.append(html.P(f\"Other visitors with '{tag}' tag:\", className=\"mb-1\"))\n",
        "                similar_visitors_list = []\n",
        "                for _, row in similar_visitors.head(5).iterrows(): \n",
        "                    similar_visitors_list.append(html.Li(f\"ID: {row['visitor_id']} (Company: {row['company']})\"))\n",
        "                tooltip_content.append(html.Ul(similar_visitors_list))\n",
        "            \n",
        "            tag_badges.append(\n",
        "                dbc.Tooltip(\n",
        "                    html.Div(tooltip_content),\n",
        "                    target=badge_id,\n",
        "                    placement=\"top\",\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        return dbc.Card(\n",
        "            dbc.CardBody([\n",
        "                dbc.Row([\n",
        "                    dbc.Col([\n",
        "                        html.H3(f'Detailed Profile for Visitor ID: {selected_visitor_id}'),\n",
        "                        html.P(f\"**Visitor Company**: {visitor_company}\", className=\"lead\")\n",
        "                    ], width=12, className=\"mb-4\")\n",
        "                ]),\n",
        "                dbc.Row([\n",
        "                    dbc.Col([\n",
        "                        dbc.Card([\n",
        "                            dbc.CardHeader(\"AI-Generated Visitor Story\"),\n",
        "                            dbc.CardBody(dcc.Markdown(user_story, className=\"story-text\"))\n",
        "                        ], className=\"mb-4\"),\n",
        "                        dbc.Card([\n",
        "                            dbc.CardHeader(\"Visitor vs. Overall Behavior Comparison\"),\n",
        "                            dbc.CardBody(dash_table.DataTable(\n",
        "                                id='comparison-table',\n",
        "                                columns=[{\"name\": i, \"id\": i} for i in profile_data.keys()],\n",
        "                                data=pd.DataFrame(profile_data).to_dict('records'),\n",
        "                                style_cell={'textAlign': 'left'},\n",
        "                            ))\n",
        "                        ]),\n",
        "                    ], md=6),\n",
        "                    dbc.Col([\n",
        "                        dbc.Card([\n",
        "                            dbc.CardHeader(\"Visitor Tags\"),\n",
        "                            dbc.CardBody(tag_badges),\n",
        "                        ], className=\"mb-4\"),\n",
        "                        dbc.Card([\n",
        "                            dbc.CardHeader(\"Visitor Path Flow (Sankey Diagram)\"),\n",
        "                            dbc.CardBody(dcc.Graph(figure=sankey_fig))\n",
        "                        ], className=\"mb-4\"),\n",
        "                        dbc.Card([\n",
        "                            dbc.CardHeader(\"Visual Behavior Comparison (Normalized)\"),\n",
        "                            dbc.CardBody(dcc.Graph(figure=radar_fig))\n",
        "                        ])\n",
        "                    ], md=6),\n",
        "                ]),\n",
        "            ])\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return html.Div([\n",
        "            html.H4(\"An error occurred while displaying visitor details.\"),\n",
        "            html.P(f\"Error: {e}\"),\n",
        "            html.P(\"This might be due to incomplete data for the selected visitor.\")\n",
        "        ])\n",
        "        \n",
        "## --- NEW CALLBACK: Control Visibility of Global Modal Buttons ---\n",
        "@app.callback(\n",
        "    Output(\"open-tag-glossary-modal-website\", \"style\"),\n",
        "    Output(\"open-cluster-glossary-modal-website-tab\", \"style\"),\n",
        "    Output(\"open-cluster-glossary-modal-visitor-tab\", \"style\"),\n",
        "    Input(\"tabs-main\", \"active_tab\")\n",
        ")\n",
        "def control_modal_button_visibility(active_tab):\n",
        "    style_show = {'display': 'inline-block', 'margin-left': '8px'} # Use inline-block for buttons\n",
        "    style_hide = {'display': 'none'}\n",
        "\n",
        "    tag_modal_button_style = style_hide\n",
        "    cluster_website_modal_button_style = style_hide\n",
        "    cluster_visitor_modal_button_style = style_hide\n",
        "\n",
        "    if active_tab == 'tab-website-company':\n",
        "        tag_modal_button_style = style_show\n",
        "        cluster_website_modal_button_style = style_show\n",
        "    elif active_tab == 'tab-visitor-company':\n",
        "        cluster_visitor_modal_button_style = style_show\n",
        "    \n",
        "    return tag_modal_button_style, cluster_website_modal_button_style, cluster_visitor_modal_button_style\n",
        "\n",
        "\n",
        "## --- NEW CALLBACKS FOR GLOSSARY MODALS ---\n",
        "# Tag Glossary Modal Toggle\n",
        "@app.callback(\n",
        "    Output(\"tag-glossary-modal\", \"is_open\"),\n",
        "    Output(\"tag-glossary-modal-body\", \"children\"),\n",
        "    Input(\"open-tag-glossary-modal-website\", \"n_clicks\"),\n",
        "    Input(\"close-tag-glossary-modal\", \"n_clicks\"),\n",
        "    State(\"tag-glossary-modal\", \"is_open\"),\n",
        ")\n",
        "def toggle_tag_glossary_modal(n_open, n_close, is_open):\n",
        "    ctx = dash.callback_context\n",
        "    if not ctx.triggered:\n",
        "        return is_open, []\n",
        "\n",
        "    button_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
        "\n",
        "    if button_id == \"open-tag-glossary-modal-website\":\n",
        "        modal_content = []\n",
        "        for tag, explanation in tag_explanations.items():\n",
        "            modal_content.append(html.H5(tag, className=\"mt-3\"))\n",
        "            modal_content.append(html.P(explanation))\n",
        "        return True, modal_content\n",
        "    elif button_id == \"close-tag-glossary-modal\":\n",
        "        return False, []\n",
        "    return is_open, []\n",
        "\n",
        "\n",
        "# Cluster Glossary Modal Toggle\n",
        "@app.callback(\n",
        "    Output(\"cluster-glossary-modal\", \"is_open\"),\n",
        "    Output(\"cluster-glossary-modal-body\", \"children\"),\n",
        "    Input(\"open-cluster-glossary-modal-website-tab\", \"n_clicks\"), \n",
        "    Input(\"open-cluster-glossary-modal-visitor-tab\", \"n_clicks\"), \n",
        "    Input(\"close-cluster-glossary-modal\", \"n_clicks\"),\n",
        "    State(\"cluster-glossary-modal\", \"is_open\"),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "def toggle_cluster_glossary_modal(n_open_website, n_open_visitor, n_close, is_open): \n",
        "    global cluster_personas_data\n",
        "    ctx = dash.callback_context\n",
        "    if not ctx.triggered:\n",
        "        return is_open, html.Div(\"No detailed cluster personas available.\") # Provide a default message content\n",
        "\n",
        "    button_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
        "\n",
        "    if button_id in [\"open-cluster-glossary-modal-website-tab\", \"open-cluster-glossary-modal-visitor-tab\"]:\n",
        "        modal_content = generate_cluster_full_description_content() \n",
        "        return True, modal_content\n",
        "    elif button_id == \"close-cluster-glossary-modal\":\n",
        "        return False, html.Div(\"No detailed cluster personas available.\") # Also return default message when closing\n",
        "    return is_open, html.Div(\"No detailed cluster personas available.\") # Default for other cases\n",
        "\n",
        "# --- Callback for Event Impact Analysis Tab ---\n",
        "@app.callback(\n",
        "    Output('event-impact-ai-summary', 'children'),\n",
        "    Output('company_a-visits-time-series', 'figure'),\n",
        "    Output('company_a-downloads-time-series', 'figure'),\n",
        "    Output('company_a-event-summary-table', 'data'),\n",
        "    Output('company_a-event-summary-table', 'columns'),\n",
        "    Output('cluster-event-impact-heatmap', 'figure'),\n",
        "    Output('cluster-event-impact-table', 'children'),\n",
        "    Output('cluster-content-interest-chart', 'figure'),\n",
        "    Input('tabs-main', 'active_tab')\n",
        ")\n",
        "def update_event_impact_tab(active_tab):\n",
        "    global cluster_personas_data, df_events, df_event_impact_summary, df_daily_vodafone_behavior_clustered, df_daily_vodafone_behavior_overall, df_all_with_profiles # 确保 df_all_with_profiles 也在这里声明为 global\n",
        "\n",
        "    if active_tab != 'tab-event-impact':\n",
        "        return (\n",
        "            \"\", # 1. AI Summary\n",
        "            go.Figure().update_layout(title=\"Select 'Event Impact Analysis' tab to view\"), # 2. Visits Figure\n",
        "            go.Figure().update_layout(title=\"Select 'Event Impact Analysis' tab to view\"), # 3. Downloads Figure\n",
        "            [], # 4. Event Summary Table Data\n",
        "            [], # 5. Event Summary Table Columns\n",
        "            go.Figure().update_layout(title=\"Select 'Event Impact Analysis' tab to view\"), # 6. Heatmap\n",
        "            html.Div(), # 7. Cluster Impact Table (empty Div)\n",
        "            go.Figure().update_layout(title=\"Select 'Event Impact Analysis' tab to view\") # 8. cluster-content-interest-chart (新增的占位符)\n",
        "        )\n",
        "\n",
        "    print(f\"\\n--- Debugging update_event_impact_tab ---\")\n",
        "    print(f\"df_events shape: {df_events.shape if not df_events.empty else 'Empty'}\")\n",
        "    print(f\"df_event_impact_summary shape: {df_event_impact_summary.shape if not df_event_impact_summary.empty else 'Empty'}\")\n",
        "    print(f\"df_daily_company_a_behavior_clustered shape: {df_daily_company_a_behavior_clustered.shape if not df_daily_company_a_behavior_clustered.empty else 'Empty'}\")\n",
        "    print(f\"df_daily_company_a_behavior_overall shape: {df_daily_company_a_behavior_overall.shape if not df_daily_company_a_behavior_overall.empty else 'Empty'}\")\n",
        "    print(f\"cluster_personas_data length: {len(cluster_personas_data) if cluster_personas_data else 'Empty'}\")\n",
        "\n",
        "\n",
        "    df_vodafone_for_analysis = df_all_with_profiles[df_all_with_profiles['website_company'] == 'Vodafone'].copy()\n",
        "    if df_vodafone_for_analysis.empty:\n",
        "       \n",
        "        ai_summary = \"No Company A data available for analysis in Event Impact tab.\"\n",
        "        fig_visits = go.Figure().update_layout(title=\"No Company A data for Visits plot.\")\n",
        "        fig_downloads = go.Figure().update_layout(title=\"No Company A data for Downloads plot.\")\n",
        "        table_data = []\n",
        "        table_columns = []\n",
        "        heatmap_fig = go.Figure().update_layout(title=\"No Company A data for Heatmap.\")\n",
        "        cluster_impact_table_content = html.Div(\"No Company A data for Detailed Cluster Event Impact.\")\n",
        "        fig_cluster_content_interest = go.Figure().update_layout(title=\"No Company A data for Content Interest Chart.\")\n",
        "        return ai_summary, fig_visits, fig_downloads, table_data, table_columns, heatmap_fig, cluster_impact_table_content, fig_cluster_content_interest\n",
        "    analyzer_for_tab = BehaviorAnalyzer(df_vodafone_for_analysis)\n",
        "\n",
        "\n",
        "    # Generate AI Summary\n",
        "    if df_event_impact_summary.empty:\n",
        "        ai_summary = \"No event impact data available to generate insights. Please ensure 'company_a_event_impact_summary.csv' is present and contains data.\"\n",
        "    else:\n",
        "        ai_summary = generate_event_ai_analysis(df_event_impact_summary)\n",
        "\n",
        "    fig_visits = go.Figure()\n",
        "    fig_downloads = go.Figure()\n",
        "    \n",
        "    fig_cluster_content_interest = go.Figure().update_layout(title=\"No content interest data by cluster available.\")\n",
        "    \n",
        "    if not df_daily_vodafone_behavior_overall.empty:\n",
        "        fig_visits = px.line(df_daily_vodafone_behavior_overall, x='date', y='total_visits',\n",
        "                             title='Company A Daily Total Visits Over Time with Financial Events',\n",
        "                             labels={'date': 'Date', 'total_visits': 'Total Unique Visits'},\n",
        "                             template='plotly_white')\n",
        "        fig_downloads = px.line(df_daily_vodafone_behavior_overall, x='date', y='total_downloads',\n",
        "                                 title='Company A Daily Total Downloads Over Time with Financial Events',\n",
        "                                 labels={'date': 'Date', 'total_downloads': 'Total Downloads'},\n",
        "                                 template='plotly_white')\n",
        "\n",
        "        # Add event markers to the plots\n",
        "        if not df_events.empty: # Only add markers if event data exists\n",
        "            for index, event in df_events.iterrows():\n",
        "                event_datetime = event['event_date'].to_pydatetime()\n",
        "                event_unix_timestamp_ms = event_datetime.timestamp() * 1000 # Convert to milliseconds for Plotly\n",
        "\n",
        "                # Add to Visits plot\n",
        "                fig_visits.add_vline(x=event_unix_timestamp_ms, line_width=1, line_dash=\"dash\", line_color=\"red\",\n",
        "                                     annotation_text=event['event_title'], annotation_position=\"top right\",\n",
        "                                     annotation_font_size=10, annotation_font_color=\"red\")\n",
        "                \n",
        "                # Add to Downloads plot\n",
        "                fig_downloads.add_vline(x=event_unix_timestamp_ms, line_width=1, line_dash=\"dash\", line_color=\"blue\",\n",
        "                                        annotation_text=event['event_title'], annotation_position=\"top right\",\n",
        "                                        annotation_font_size=10, annotation_font_color=\"blue\")\n",
        "        \n",
        "        fig_visits.update_layout(hovermode=\"x unified\")\n",
        "        fig_downloads.update_layout(hovermode=\"x unified\")\n",
        "    else:\n",
        "        fig_visits = go.Figure().update_layout(title=\"No daily Vodafone behavior data available for Visits plot.\")\n",
        "        fig_downloads = go.Figure().update_layout(title=\"No daily Vodafone behavior data available for Downloads plot.\")\n",
        "\n",
        "    # 3. Prepare Event Summary Table Data\n",
        "    table_data = df_event_impact_summary.to_dict('records') if not df_event_impact_summary.empty else []\n",
        "    table_columns = [{\"name\": col, \"id\": col} for col in df_event_impact_summary.columns] if not df_event_impact_summary.empty else []\n",
        "\n",
        "    # 4. Prepare Cluster-level Event Impact Analysis\n",
        "    cluster_event_impact_data = []\n",
        "    if not df_daily_vodafone_behavior_clustered.empty and not df_events.empty and cluster_personas_data:\n",
        "        pre_event_window = 7\n",
        "        post_event_window = 7\n",
        "        baseline_offset = 20 # Days before pre-event window starts\n",
        "        baseline_duration = 10 # Duration of baseline window\n",
        "\n",
        "        for _, event in df_events.iterrows():\n",
        "            event_date = event['event_date']\n",
        "            event_title = event['event_title']\n",
        "            \n",
        "            event_window_start = event_date - timedelta(days=pre_event_window)\n",
        "            event_window_end = event_date + timedelta(days=post_event_window)\n",
        "            \n",
        "            baseline_start = event_date - timedelta(days=pre_event_window + baseline_offset)\n",
        "            baseline_end = event_date - timedelta(days=pre_event_window + baseline_offset - 1 + baseline_duration)\n",
        "\n",
        "            event_window_behavior = df_daily_vodafone_behavior_clustered[\n",
        "                (df_daily_vodafone_behavior_clustered['date'] >= event_window_start) & \n",
        "                (df_daily_vodafone_behavior_clustered['date'] <= event_window_end)\n",
        "            ]\n",
        "            baseline_behavior = df_daily_vodafone_behavior_clustered[\n",
        "                (df_daily_vodafone_behavior_clustered['date'] >= baseline_start) & \n",
        "                (df_daily_vodafone_behavior_clustered['date'] <= baseline_end)\n",
        "            ]\n",
        "\n",
        "            print(f\"  Processing event: {event_title} ({event_date.strftime('%Y-%m-%d')})\")\n",
        "            print(f\"    Event window behavior rows: {len(event_window_behavior)}\")\n",
        "            print(f\"    Baseline behavior rows: {len(baseline_behavior)}\")\n",
        "\n",
        "            avg_event_metrics = event_window_behavior.groupby('cluster').agg(\n",
        "                avg_visits=('total_visits', 'mean'),\n",
        "                avg_downloads=('total_downloads', 'mean'),\n",
        "                avg_session_depth=('avg_session_depth', 'mean')\n",
        "            )\n",
        "            avg_baseline_metrics = baseline_behavior.groupby('cluster').agg(\n",
        "                avg_visits=('total_visits', 'mean'),\n",
        "                avg_downloads=('total_downloads', 'mean'),\n",
        "                avg_session_depth=('avg_session_depth', 'mean')\n",
        "            )\n",
        "\n",
        "            combined_metrics = avg_event_metrics.join(avg_baseline_metrics, how='outer', \n",
        "                                                     lsuffix='_event', rsuffix='_baseline').fillna(0)\n",
        "\n",
        "            for cluster_id in combined_metrics.index.unique(): # Use .unique() to avoid duplicates\n",
        "                cluster_name = f\"Cluster {int(cluster_id)}\" # Ensure cluster_id is int for formatting\n",
        "                for persona in cluster_personas_data:\n",
        "                    if persona['num'] == cluster_id: # cluster_id from df is int, persona['num'] is int\n",
        "                        cluster_name = persona['name']\n",
        "                        break\n",
        "\n",
        "                avg_visits_event = combined_metrics.loc[cluster_id, 'avg_visits_event']\n",
        "                avg_visits_baseline = combined_metrics.loc[cluster_id, 'avg_visits_baseline']\n",
        "                \n",
        "                avg_downloads_event = combined_metrics.loc[cluster_id, 'avg_downloads_event']\n",
        "                avg_downloads_baseline = combined_metrics.loc[cluster_id, 'avg_downloads_baseline']\n",
        "                \n",
        "                avg_session_depth_event = combined_metrics.loc[cluster_id, 'avg_session_depth_event']\n",
        "                avg_session_depth_baseline = combined_metrics.loc[cluster_id, 'avg_session_depth_baseline']\n",
        "\n",
        "                # Recalculate percentage changes, explicitly setting to NaN if baseline is 0\n",
        "                visits_change_pct = ((avg_visits_event - avg_visits_baseline) / avg_visits_baseline * 100) if avg_visits_baseline != 0 else np.nan\n",
        "                downloads_change_pct = ((avg_downloads_event - avg_downloads_baseline) / avg_downloads_baseline * 100) if avg_downloads_baseline != 0 else np.nan\n",
        "                session_depth_change_pct = ((avg_session_depth_event - avg_session_depth_baseline) / avg_session_depth_baseline * 100) if avg_session_depth_baseline != 0 else np.nan\n",
        "                \n",
        "                # --- Debugging: Check calculated percentages ---\n",
        "                print(f\"    Cluster {cluster_id} ({cluster_name}) changes for event {event_title}:\")\n",
        "                print(f\"      Visits change: {visits_change_pct:.2f}% (Event: {avg_visits_event:.2f}, Baseline: {avg_visits_baseline:.2f})\")\n",
        "                print(f\"      Downloads change: {downloads_change_pct:.2f}% (Event: {avg_downloads_event:.2f}, Baseline: {avg_downloads_baseline:.2f})\")\n",
        "                print(f\"      Session Depth change: {session_depth_change_pct:.2f}% (Event: {avg_session_depth_event:.2f}, Baseline: {avg_session_depth_baseline:.2f})\")\n",
        "\n",
        "\n",
        "                cluster_event_impact_data.append({\n",
        "                    'Event Title': event_title,\n",
        "                    'Event Date': event_date.strftime('%Y-%m-%d'),\n",
        "                    'Cluster ID': int(cluster_id), # Store as int\n",
        "                    'Cluster Name': cluster_name,\n",
        "                    'Change in Visits (%)': round(visits_change_pct, 2) if pd.notna(visits_change_pct) else np.nan,\n",
        "                    'Change in Downloads (%)': round(downloads_change_pct, 2) if pd.notna(downloads_change_pct) else np.nan,\n",
        "                    'Change in Session Depth (%)': round(session_depth_change_pct, 2) if pd.notna(session_depth_change_pct) else np.nan\n",
        "                })\n",
        "\n",
        "    df_cluster_event_impact = pd.DataFrame(cluster_event_impact_data)\n",
        "    \n",
        "    # Ensure numeric columns for pivot_table, handling NaNs\n",
        "    for col_name in ['Change in Visits (%)', 'Change in Downloads (%)', 'Change in Session Depth (%)']:\n",
        "        if col_name in df_cluster_event_impact.columns:\n",
        "            df_cluster_event_impact[col_name] = pd.to_numeric(df_cluster_event_impact[col_name], errors='coerce')\n",
        "\n",
        "\n",
        "    # 5. Visualization: Heatmap for Cluster Impact\n",
        "    heatmap_fig = go.Figure().update_layout(title=\"No sufficient clustered event impact data or all values are NaN for heatmap.\") # Default empty figure with message\n",
        "    \n",
        "    if not df_cluster_event_impact.empty and 'Change in Visits (%)' in df_cluster_event_impact.columns and not df_cluster_event_impact['Change in Visits (%)'].isnull().all():\n",
        "        df_for_heatmap = df_cluster_event_impact.dropna(subset=['Change in Visits (%)'])\n",
        "        \n",
        "        if not df_for_heatmap.empty:\n",
        "            heatmap_data = df_for_heatmap.pivot_table(\n",
        "                index='Cluster Name', columns='Event Title', values='Change in Visits (%)'\n",
        "            )\n",
        "            \n",
        "            if not df_event_impact_summary.empty:\n",
        "                sorted_event_titles = df_event_impact_summary.sort_values('Event Date')['Event Title'].tolist()\n",
        "                actual_event_titles_in_heatmap = [title for title in sorted_event_titles if title in heatmap_data.columns]\n",
        "                # Filter heatmap_data to only include columns that are actually present AND sorted\n",
        "                heatmap_data = heatmap_data[actual_event_titles_in_heatmap] \n",
        "            else:\n",
        "                heatmap_data = heatmap_data[heatmap_data.columns.tolist()] # Reorder based on current columns\n",
        "\n",
        "            heatmap_fig = px.heatmap(\n",
        "                heatmap_data,\n",
        "                title='Change in Visits (%) by Investor Cluster Around Financial Events',\n",
        "                labels={'x': 'Event Title', 'y': 'Investor Cluster', 'color': 'Change in Visits (%)'},\n",
        "                color_continuous_scale=px.colors.sequential.RdBu,\n",
        "                color_continuous_midpoint=0,\n",
        "                height=400\n",
        "            )\n",
        "            heatmap_fig.update_xaxes(tickangle=45) \n",
        "            # Add text on cells for better readability (optional)\n",
        "            heatmap_fig.update_traces(texttemplate=\"%{z:.2f}%\", textfont_size=10)\n",
        "        else:\n",
        "            heatmap_fig = go.Figure().update_layout(title=\"All 'Change in Visits (%)' values are NaN after filtering for heatmap.\")\n",
        "\n",
        "\n",
        "    # 6. Display Cluster Impact in a Table\n",
        "    cluster_impact_table_content = html.Div([\n",
        "        html.H4(\"Detailed Investor Cluster Behavior Changes Around Events\"),\n",
        "        html.P(\"This table shows the percentage change in key metrics for each investor cluster around specific Vodafone events compared to a baseline period.\"),\n",
        "        dash_table.DataTable(\n",
        "            id='cluster-event-impact-detail-table',\n",
        "            columns=[{\"name\": col, \"id\": col} for col in df_cluster_event_impact.columns],\n",
        "            data=df_cluster_event_impact.to_dict('records'),\n",
        "            style_table={'overflowX': 'auto'},\n",
        "            page_action=\"native\",\n",
        "            page_current=0,\n",
        "            page_size=10,\n",
        "            sort_action=\"native\",\n",
        "            sort_mode=\"multi\",\n",
        "            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},\n",
        "            style_cell={'textAlign': 'left', 'minWidth': '100px', 'width': '100px', 'maxWidth': '180px'},\n",
        "        ) if not df_cluster_event_impact.empty else html.P(\"No detailed cluster event impact data available.\")\n",
        "    ])\n",
        "\n",
        "    # --- FINAL RETURN STATEMENT (THIS MUST RETURN 8 ELEMENTS) ---\n",
        "    return ai_summary, fig_visits, fig_downloads, table_data, table_columns, heatmap_fig, cluster_impact_table_content, fig_cluster_content_interest\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # During development/debugging, keep this for easier traceback.\n",
        "    # For production, consider removing it to allow Dash to expose client-side errors.\n",
        "    app.config.suppress_callback_exceptions = True \n",
        "    from IPython import get_ipython\n",
        "    if get_ipython() is not None:\n",
        "        get_ipython().run_line_magic('config', 'InteractiveShell.xmode = \"Plain\"')\n",
        "        \n",
        "    # --- Instantiate and train the predictive model here ---\n",
        "    # You need to make sure the PredictiveModeler class is defined earlier in your script.\n",
        "    predictive_modeler = PredictiveModeler(df_processed)\n",
        "    df_longitudinal_data = predictive_modeler.prepare_longitudinal_data()\n",
        "    gbm_model, gbm_metrics = predictive_modeler.train_and_evaluate_gbm(df_longitudinal_data)\n",
        "\n",
        "    if gbm_model is not None:\n",
        "        print(\"\\n--- Predictive Model Training Complete ---\")\n",
        "        print(\"Model Performance Metrics (from Time-Series Cross-Validation):\")\n",
        "        print(f\"Mean AUC: {gbm_metrics['mean_auc']:.4f}\")\n",
        "        print(f\"Mean MAE: {gbm_metrics['mean_mae']:.4f}\")\n",
        "        # Optional: You can store these metrics in the overall_metrics dictionary\n",
        "        overall_metrics['predictive_model_metrics'] = gbm_metrics\n",
        "    else:\n",
        "        print(\"\\n--- Predictive Model Training Failed ---\")\n",
        "        print(f\"Reason: {gbm_metrics}\")\n",
        "    \n",
        "    threading.Thread(target=open_browser_after_startup).start()\n",
        "    app.run(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
