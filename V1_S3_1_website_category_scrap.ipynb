{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\yvonn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "%pip install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
        "SERPER_API_KEY = os.environ.get('SERPER_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY is not set\")\n",
        "if not SERPER_API_KEY:\n",
        "    raise ValueError(\"SERPER_API_KEY is not set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filtered saved as filtered_traffic_data.csv，left 127206 rows\n"
          ]
        }
      ],
      "source": [
        "# EDA and data cleaning, saved as filtered_traffic_data.csv\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('transformed_traffic_data.csv')\n",
        "\n",
        "drop_cols = [\n",
        "    'uri', 'query_parameters', 'headless_user_agent', 'session_stats_events',\n",
        "    'session_stats_unique_pages', 'total_session_time_minutes', 'ip_stats_events_30d',\n",
        "    'ip_stats_unique_pages_30d', 'asn_report_name', 'asn_description',\n",
        "    'referrer_original', 'previous_timestamp', 'last_ip_visit_timestamp', 'last_updated_date'\n",
        "]\n",
        "df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
        "df = df[df['match_company_id'].notna()]\n",
        "\n",
        "df.to_csv('filtered_traffic_data.csv', index=False)\n",
        "print(f\"filtered saved as filtered_traffic_data.csv，left {len(df)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 exists, skipping...\n",
            "Batch 2 exists, skipping...\n",
            "Batch 3 exists, skipping...\n",
            "Batch 4 exists, skipping...\n",
            "Batch 5 exists, skipping...\n",
            "Batch 6 exists, skipping...\n",
            "Batch 7 exists, skipping...\n",
            "Batch 8 exists, skipping...\n",
            "Batch 9 exists, skipping...\n",
            "Batch 10 exists, skipping...\n",
            "Batch 11 exists, skipping...\n",
            "Batch 12 exists, skipping...\n",
            "Batch 13 exists, skipping...\n",
            "Batch 14 exists, skipping...\n",
            "Batch 15 exists, skipping...\n",
            "Batch 16 exists, skipping...\n",
            "Batch 17 exists, skipping...\n",
            "Batch 18 exists, skipping...\n",
            "Batch 19 exists, skipping...\n",
            "Batch 20 exists, skipping...\n",
            "Batch 21 exists, skipping...\n",
            "Batch 22 exists, skipping...\n",
            "Batch 23 exists, skipping...\n",
            "Batch 24 exists, skipping...\n",
            "Batch 25 exists, skipping...\n",
            "Batch 26 exists, skipping...\n",
            "Batch 27 exists, skipping...\n",
            "Batch 28 exists, skipping...\n",
            "Batch 29 exists, skipping...\n",
            "Batch 30 exists, skipping...\n",
            "Batch 31 exists, skipping...\n",
            "Batch 32 exists, skipping...\n",
            "Batch 33 exists, skipping...\n",
            "Batch 34 exists, skipping...\n",
            "Batch 35 exists, skipping...\n",
            "Batch 36 exists, skipping...\n",
            "Batch 37 exists, skipping...\n",
            "Batch 38 exists, skipping...\n",
            "Batch 39 exists, skipping...\n",
            "Batch 40 exists, skipping...\n",
            "Batch 41 exists, skipping...\n",
            "Batch 42 exists, skipping...\n",
            "Batch 43 exists, skipping...\n",
            "Batch 44 exists, skipping...\n",
            "Batch 45 exists, skipping...\n",
            "Batch 46 exists, skipping...\n",
            "Batch 47 exists, skipping...\n",
            "Batch 48 exists, skipping...\n",
            "Batch 49 exists, skipping...\n",
            "Batch 50 exists, skipping...\n",
            "Batch 51 exists, skipping...\n",
            "Batch 52 exists, skipping...\n",
            "Batch 53 exists, skipping...\n",
            "Batch 54 exists, skipping...\n",
            "Batch 55 exists, skipping...\n",
            "Batch 56 exists, skipping...\n",
            "Batch 57 exists, skipping...\n",
            "Batch 58 exists, skipping...\n",
            "Batch 59 exists, skipping...\n",
            "Batch 60 exists, skipping...\n",
            "Batch 61 exists, skipping...\n",
            "Batch 62 exists, skipping...\n",
            "Batch 63 exists, skipping...\n",
            "Batch 64 exists, skipping...\n",
            "Batch 65 exists, skipping...\n",
            "Batch 66 exists, skipping...\n",
            "Batch 67 exists, skipping...\n",
            "Batch 68 exists, skipping...\n",
            "Batch 69 exists, skipping...\n",
            "Batch 70 exists, skipping...\n",
            "Batch 71 exists, skipping...\n",
            "Batch 72 exists, skipping...\n",
            "Batch 73 exists, skipping...\n",
            "Batch 74 exists, skipping...\n",
            "Batch 75 exists, skipping...\n",
            "Batch 76 exists, skipping...\n",
            "Batch 77 exists, skipping...\n",
            "Batch 78 exists, skipping...\n",
            "Batch 79 exists, skipping...\n",
            "Batch 80 exists, skipping...\n",
            "Batch 81 exists, skipping...\n",
            "Batch 82 exists, skipping...\n",
            "Batch 83 exists, skipping...\n",
            "Batch 84 exists, skipping...\n",
            "Batch 85 exists, skipping...\n",
            "Batch 86 exists, skipping...\n",
            "Batch 87 exists, skipping...\n",
            "Batch 88 exists, skipping...\n",
            "Batch 89 exists, skipping...\n",
            "Batch 90 exists, skipping...\n",
            "Batch 91 exists, skipping...\n",
            "Batch 92 exists, skipping...\n",
            "Batch 93 exists, skipping...\n",
            "Batch 94 exists, skipping...\n",
            "Batch 95 exists, skipping...\n",
            "Batch 96 exists, skipping...\n",
            "Batch 97 exists, skipping...\n",
            "Batch 98 exists, skipping...\n",
            "Batch 99 exists, skipping...\n",
            "Batch 100 exists, skipping...\n",
            "Batch 101 exists, skipping...\n",
            "Batch 102 exists, skipping...\n",
            "Batch 103 exists, skipping...\n",
            "Batch 104 exists, skipping...\n",
            "Batch 105 exists, skipping...\n",
            "Batch 106 exists, skipping...\n",
            "Batch 107 exists, skipping...\n",
            "Batch 108 exists, skipping...\n",
            "Batch 109 exists, skipping...\n",
            "Batch 110 exists, skipping...\n",
            "Batch 111 exists, skipping...\n",
            "Batch 112 exists, skipping...\n",
            "Batch 113 exists, skipping...\n",
            "Batch 114 exists, skipping...\n",
            "Batch 115 exists, skipping...\n",
            "Batch 116 exists, skipping...\n",
            "Batch 117 exists, skipping...\n",
            "Batch 118 exists, skipping...\n",
            "Batch 119 exists, skipping...\n",
            "Batch 120 exists, skipping...\n",
            "Batch 121 exists, skipping...\n",
            "Batch 122 exists, skipping...\n",
            "Batch 123 exists, skipping...\n",
            "Batch 124 exists, skipping...\n",
            "Batch 125 exists, skipping...\n",
            "Batch 126 exists, skipping...\n",
            "Batch 127 exists, skipping...\n",
            "Batch 128 exists, skipping...\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies if needed\n",
        "# %pip install lxml beautifulsoup4 google-generativeai\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# 1. Load your universal website category framework\n",
        "with open(\"universal_website_category_framework.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    framework = json.load(f)\n",
        "\n",
        "section_subsections = []\n",
        "for section, subs in framework.items():\n",
        "    for sub in subs:\n",
        "        section_subsections.append((section, sub))\n",
        "\n",
        "# 2. Input company name or URL\n",
        "def is_url(s):\n",
        "    url_pattern = re.compile(\n",
        "        r'^(http://|https://|www\\.)|([a-zA-Z0-9-]+\\.[a-zA-Z]{2,})'\n",
        "    )\n",
        "    return bool(url_pattern.match(s.strip()))\n",
        "\n",
        "def normalize_url(url):\n",
        "    url = url.strip()\n",
        "    if url.startswith(\"http://\") or url.startswith(\"https://\"):\n",
        "        return url\n",
        "    elif url.startswith(\"www.\"):\n",
        "        return \"http://\" + url\n",
        "    else:\n",
        "        return \"http://\" + url\n",
        "\n",
        "# 3. Search for sitemap URL\n",
        "def search_sitemap(company_name):\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = json.dumps({\"q\": f\"{company_name} sitemap\"})\n",
        "    headers = {\n",
        "        'X-API-KEY': SERPER_API_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    search_results = json.loads(response.text)\n",
        "    candidate_urls = []\n",
        "    if 'organic' in search_results and len(search_results['organic']) > 0:\n",
        "        for result in search_results['organic']:\n",
        "            candidate_urls.append(result['link'])\n",
        "    suffixes = ['', '/sitemap.xml', '/site-map', '/sitemap/']\n",
        "    for base_url in candidate_urls:\n",
        "        for suffix in suffixes:\n",
        "            test_url = base_url.rstrip('/') + suffix\n",
        "            try:\n",
        "                resp = requests.get(test_url, timeout=5)\n",
        "                if resp.status_code == 200:\n",
        "                    return test_url\n",
        "            except Exception:\n",
        "                continue\n",
        "    return None\n",
        "\n",
        "# 4. Fetch URLs from sitemap (your function, works for XML/HTML)\n",
        "def fetch_urls_from_sitemap(sitemap_url, max_links=100):\n",
        "    urls = []\n",
        "    try:\n",
        "        resp = requests.get(sitemap_url, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"Failed to fetch {sitemap_url}\")\n",
        "            return urls\n",
        "        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
        "        text = resp.text\n",
        "        # Try XML first\n",
        "        if sitemap_url.endswith('.xml') or 'xml' in content_type:\n",
        "            soup = BeautifulSoup(text, \"lxml-xml\")\n",
        "            sitemap_tags = soup.find_all(\"sitemap\")\n",
        "            if sitemap_tags:\n",
        "                # Sitemap index\n",
        "                for sitemap in sitemap_tags:\n",
        "                    loc = sitemap.find(\"loc\")\n",
        "                    if loc and len(urls) < max_links:\n",
        "                        child_urls = fetch_urls_from_sitemap(loc.text, max_links=max_links-len(urls))\n",
        "                        urls += child_urls\n",
        "                        if len(urls) >= max_links:\n",
        "                            break\n",
        "            else:\n",
        "                # Regular XML sitemap\n",
        "                for loc in soup.find_all(\"loc\"):\n",
        "                    if len(urls) < max_links:\n",
        "                        urls.append(loc.text)\n",
        "                    else:\n",
        "                        break\n",
        "        else:\n",
        "            # Try HTML sitemap\n",
        "            soup = BeautifulSoup(text, \"lxml\")\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                href = a['href']\n",
        "                if href.startswith(\"http\"):\n",
        "                    urls.append(href)\n",
        "                elif href.startswith(\"/\"):\n",
        "                    full_url = urljoin(sitemap_url, href)\n",
        "                    urls.append(full_url)\n",
        "                if len(urls) >= max_links:\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching/parsing sitemap: {e}\")\n",
        "    return list(set(urls))\n",
        "\n",
        "# 5. Prepare mapping examples from the universal framework\n",
        "mapping_examples = []\n",
        "for section, subs in framework.items():\n",
        "    for sub in subs:\n",
        "        mapping_examples.append(f\"  '{section} > {sub}' -> section: {section}, subsection: {sub}\")\n",
        "    if not subs:\n",
        "        mapping_examples.append(f\"  '{section}' -> section: {section}, subsection: null\")\n",
        "\n",
        "# 6. LLM mapping function\n",
        "def extract_json_from_text(text):\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[7:]\n",
        "    if text.startswith(\"```\"):\n",
        "        text = text[3:]\n",
        "    if text.endswith(\"```\"):\n",
        "        text = text[:-3]\n",
        "    matches = re.findall(r\"\\{[\\s\\S]*?\\}\", text)\n",
        "    for m in matches:\n",
        "        try:\n",
        "            return json.loads(m)\n",
        "        except Exception:\n",
        "            continue\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "#for privacy concern, the company example here is deleted\n",
        "def llm_url_to_framework_category_batch(urls, section_subsections, model, max_retries=3):\n",
        "    options_str = \"\\n\".join([f\"- {sec} / {sub}\" for sec, sub in section_subsections])\n",
        "    example = (\n",
        "        '{\\n'\n",
        "        '  \"https://www.xxxxxxx.com/about-us/index\": [\"About Us\", \"About Us\"],\\n'\n",
        "        '  \"https://www.xxxxxxx.com/investors/shareholders/registration-services\": [\"Investors\", \"Investor Relations\"]\\n'\n",
        "        '}'\n",
        "    )\n",
        "    prompt = (\n",
        "        \"You are a website taxonomy expert. For the following URLs, \"\n",
        "        \"assign each to the most appropriate (section, subsection) from the universal website category framework below. \"\n",
        "        \"You MUST choose from the provided pairs, do NOT invent new ones. Do NOT use 'Other'.\\n\\n\"\n",
        "        \"Valid (section, subsection) pairs:\\n\"\n",
        "        f\"{options_str}\\n\\n\"\n",
        "        \"Output as a JSON object mapping each URL to a [section, subsection] list. Example:\\n\"\n",
        "        f\"{example}\\n\"\n",
        "        \"URL list:\\n\"\n",
        "        + \"\\n\".join([f\"- {url}\" for url in urls]) +\n",
        "        \"\\n\\nJSON:\"\n",
        "    )\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            text = response.text\n",
        "            print(\"LLM raw output:\\n\", text)\n",
        "            parsed = extract_json_from_text(text)\n",
        "            if parsed:\n",
        "                return parsed\n",
        "            else:\n",
        "                print(\"No JSON found in LLM response, retrying...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}, retrying...\")\n",
        "            time.sleep(3)\n",
        "    return {}\n",
        "\n",
        "# 7. Configure Gemini\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "BATCH_TEST = True\n",
        "\n",
        "if BATCH_TEST:\n",
        "    df = pd.read_csv('filtered_traffic_data.csv')\n",
        "    df = df[df['url'].notna()]\n",
        "    batch_size = 1000\n",
        "    total = min(200000, len(df))\n",
        "    BATCH_SIZE_PER_LLM = 20  \n",
        "\n",
        "    for batch_idx, start in enumerate(range(0, total, batch_size)):\n",
        "        end = min(start + batch_size, total)\n",
        "        outname = f'filtered_traffic_data_with_category_batch{batch_idx+1}.csv'\n",
        "        if os.path.exists(outname):\n",
        "            print(f\"Batch {batch_idx+1} exists, skipping...\")\n",
        "            continue\n",
        "        df_sample = df.iloc[start:end]\n",
        "        urls_to_classify = df_sample['url'].tolist()\n",
        "        print(f\"Batch {batch_idx+1}: classifying {len(urls_to_classify)} URLs\")\n",
        "        classified_urls = {}\n",
        "        for i in range(0, len(urls_to_classify), BATCH_SIZE_PER_LLM):\n",
        "            sub_urls = urls_to_classify[i:i+BATCH_SIZE_PER_LLM]\n",
        "            result = llm_url_to_framework_category_batch(\n",
        "                sub_urls, section_subsections, model\n",
        "            )\n",
        "            classified_urls.update(result)\n",
        "\n",
        "        df_sample['section'] = df_sample['url'].map(lambda x: classified_urls.get(x, [None, None])[0])\n",
        "        df_sample['sub-section'] = df_sample['url'].map(lambda x: classified_urls.get(x, [None, None])[1])\n",
        "        df_sample.to_csv(outname, index=False)\n",
        "        print(f\"Saved batch {batch_idx+1} to {outname}\")\n",
        "else:\n",
        "    user_input = input(\"enter company name or URL: \").strip()\n",
        "    if is_url(user_input):\n",
        "        urls_to_classify = [normalize_url(user_input)]\n",
        "        safe_company_name = \"single_url\"\n",
        "        print(f\"if enter URL，directly classify：{urls_to_classify[0]}\")\n",
        "    else:\n",
        "        company_name = user_input\n",
        "        sitemap_url = search_sitemap(company_name)\n",
        "        if not sitemap_url:\n",
        "            print(\"Could not find sitemap URL.\")\n",
        "            exit()\n",
        "        print(\"Sitemap URL:\", sitemap_url)\n",
        "        max_links = 100\n",
        "        urls_to_classify = fetch_urls_from_sitemap(sitemap_url, max_links=max_links)\n",
        "        print(f\"Found {len(urls_to_classify)} URLs in sitemap.\")\n",
        "        safe_company_name = company_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
        "    classified_urls = llm_url_to_framework_category_single(\n",
        "        urls_to_classify, section_subsections, model\n",
        "    )\n",
        "    print(f\"Classified {len(classified_urls)} URLs.\")\n",
        "    for item in list(classified_urls.items())[:10]:\n",
        "        print(item)\n",
        "    filename = f\"url_to_universal_category_{safe_company_name}.json\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(classified_urls, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "all_files = glob.glob(\"filtered_traffic_data_with_category_batch*.csv\")\n",
        "df_list = [pd.read_csv(f) for f in all_files]\n",
        "df_merged = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "\n",
        "mask_empty = df_merged['section'].isna() | (df_merged['section'] == '') | (df_merged['section'].astype(str).str.lower() == 'none')\n",
        "urls_to_retry = df_merged.loc[mask_empty, 'url'].tolist()\n",
        "\n",
        "\n",
        "if os.path.exists(\"new_sections_partial.json\"):\n",
        "    with open(\"new_sections_partial.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        new_sections = json.load(f)\n",
        "else:\n",
        "    new_sections = {}\n",
        "\n",
        "\n",
        "urls_to_retry = [url for url in urls_to_retry if url not in new_sections]\n",
        "\n",
        "BATCH_SIZE_PER_LLM = 20\n",
        "for i in range(0, len(urls_to_retry), BATCH_SIZE_PER_LLM):\n",
        "    sub_urls = urls_to_retry[i:i+BATCH_SIZE_PER_LLM]\n",
        "    result = llm_url_to_framework_category_batch(sub_urls, section_subsections, model)\n",
        "    new_sections.update(result)\n",
        "    \n",
        "    with open(\"new_sections_partial.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(new_sections, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"已处理 {i+BATCH_SIZE_PER_LLM} / {len(urls_to_retry)}\")\n",
        "\n",
        "\n",
        "def fill_section(row):\n",
        "    if (pd.isna(row['section']) or row['section'] in ['', None, 'None']) and row['url'] in new_sections:\n",
        "        return new_sections[row['url']][0]\n",
        "    return row['section']\n",
        "\n",
        "def fill_subsection(row):\n",
        "    if (pd.isna(row['sub-section']) or row['sub-section'] in ['', None, 'None']) and row['url'] in new_sections:\n",
        "        return new_sections[row['url']][1]\n",
        "    return row['sub-section']\n",
        "\n",
        "df_merged['section'] = df_merged.apply(fill_section, axis=1)\n",
        "df_merged['sub-section'] = df_merged.apply(fill_subsection, axis=1)\n",
        "df_merged.to_csv(\"all_traffic_data_merged.csv\", index=False)\n",
        "print(\"saved to all_traffic_data_merged.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in c:\\users\\yvonn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\yvonn\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (2.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Saved to all_traffic_data_merged_with_attributes.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "%pip install openpyxl\n",
        "df_merged = pd.read_csv('all_traffic_data_merged.csv')\n",
        "df_visitor = pd.read_csv('visitor_attributes.csv')\n",
        "\n",
        "visitor_cols = ['company_type-2', 'factset_industries_name', 'factset_sector_name', 'visitor_id', 'institution_id']\n",
        "df_visitor_sub = df_visitor.dropna(subset=['company_type-2', 'factset_industries_name', 'factset_sector_name'])\n",
        "\n",
        "df_merged = df_merged.merge(\n",
        "    df_visitor_sub[visitor_cols],\n",
        "    how='left',\n",
        "    left_on='visitor_id',\n",
        "    right_on='visitor_id'\n",
        ")\n",
        "\n",
        "df_inst = pd.read_excel('institution_id_attributes.xlsx')\n",
        "df_inst.columns = df_inst.columns.str.strip()\n",
        "\n",
        "df_merged['institution_id'] = df_merged['institution_id'].astype(str)\n",
        "df_inst['InstitutionID'] = df_inst['InstitutionID'].astype(str)\n",
        "\n",
        "df_merged = df_merged.merge(\n",
        "    df_inst[['InstitutionID', 'Side', 'InstitutionType', 'Country', 'Region']],\n",
        "    how='left',\n",
        "    left_on='institution_id',\n",
        "    right_on='InstitutionID'\n",
        ")\n",
        "\n",
        "df_merged['location_country'] = df_merged['Country']\n",
        "\n",
        "df_merged = df_merged.drop(columns=['InstitutionID', 'institution_id', 'Country'])\n",
        "\n",
        "df_merged.to_csv('all_traffic_data_merged_with_attributes.csv', index=False)\n",
        "print(\"Saved to all_traffic_data_merged_with_attributes.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
